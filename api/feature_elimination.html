
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Validation of regressors and classifiers and data used to develop them">
      
      
        <meta name="author" content="ING Bank N. V.">
      
      
        <link rel="canonical" href="https://ing-bank.github.io/probatus/api/feature_elimination.html">
      
      
        <link rel="prev" href="../index.html">
      
      
        <link rel="next" href="model_interpret.html">
      
      
      <link rel="icon" href="../img/Probatus_P_white.png">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.5.29">
    
    
      
        <title>Features Elimination - Probatus</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.76a95c52.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:300,300i,400,400i,700,700i%7CUbuntu+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Ubuntu";--md-code-font:"Ubuntu Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="deep-orange" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#features-elimination" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../index.html" title="Probatus" class="md-header__button md-logo" aria-label="Probatus" data-md-component="logo">
      
  <img src="../img/Probatus_P_white.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Probatus
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Features Elimination
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/ing-bank/probatus/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../index.html" class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="feature_elimination.html" class="md-tabs__link">
          
  
  Api

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../discussion/nb_rfecv_vs_shaprfecv.html" class="md-tabs__link">
          
  
  Discussion

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../howto/grouped_data.html" class="md-tabs__link">
          
  
  Howto

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../tutorials/nb_automatic_best_num_features.html" class="md-tabs__link">
          
  
  Tutorials

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../index.html" title="Probatus" class="md-nav__button md-logo" aria-label="Probatus" data-md-component="logo">
      
  <img src="../img/Probatus_P_white.png" alt="logo">

    </a>
    Probatus
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/ing-bank/probatus/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../index.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Api
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Api
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Features Elimination
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="feature_elimination.html" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Features Elimination
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#probatus.feature_elimination.feature_elimination" class="md-nav__link">
    <span class="md-ellipsis">
      feature_elimination
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#probatus.feature_elimination.feature_elimination.ShapRFECV" class="md-nav__link">
    <span class="md-ellipsis">
      ShapRFECV
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ShapRFECV">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#probatus.feature_elimination.feature_elimination.ShapRFECV.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#probatus.feature_elimination.feature_elimination.ShapRFECV.compute" class="md-nav__link">
    <span class="md-ellipsis">
      compute
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#probatus.feature_elimination.feature_elimination.ShapRFECV.fit" class="md-nav__link">
    <span class="md-ellipsis">
      fit
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#probatus.feature_elimination.feature_elimination.ShapRFECV.fit_compute" class="md-nav__link">
    <span class="md-ellipsis">
      fit_compute
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#probatus.feature_elimination.feature_elimination.ShapRFECV.get_reduced_features_set" class="md-nav__link">
    <span class="md-ellipsis">
      get_reduced_features_set
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#probatus.feature_elimination.feature_elimination.ShapRFECV.plot" class="md-nav__link">
    <span class="md-ellipsis">
      plot
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="model_interpret.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Model Interpretation using SHAP
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="sample_similarity.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Sample Similarity
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="utils.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Utility Functions
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Discussion
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Discussion
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../discussion/nb_rfecv_vs_shaprfecv.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ShapRFECV vs sklearn RFECV
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Howto
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Howto
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../howto/grouped_data.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    How to work with grouped data
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../howto/reproducibility.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    How to ensure reproducibility of the results
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Tutorials
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Tutorials
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/nb_automatic_best_num_features.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Automatic Feature selection techniques
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/nb_custom_scoring.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Custom Scoring Metrics
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/nb_sample_similarity.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Sample Similarity
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/nb_shap_dependence.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Shap dependence
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/nb_shap_feature_elimination.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ShapRFECV - Recursive Feature Elimination using SHAP importance
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/nb_shap_model_interpreter.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tree Model Interpretation using SHAP
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/nb_shap_variance_penalty_and_results_comparison.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Shap variance penalty
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#probatus.feature_elimination.feature_elimination" class="md-nav__link">
    <span class="md-ellipsis">
      feature_elimination
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#probatus.feature_elimination.feature_elimination.ShapRFECV" class="md-nav__link">
    <span class="md-ellipsis">
      ShapRFECV
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ShapRFECV">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#probatus.feature_elimination.feature_elimination.ShapRFECV.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#probatus.feature_elimination.feature_elimination.ShapRFECV.compute" class="md-nav__link">
    <span class="md-ellipsis">
      compute
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#probatus.feature_elimination.feature_elimination.ShapRFECV.fit" class="md-nav__link">
    <span class="md-ellipsis">
      fit
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#probatus.feature_elimination.feature_elimination.ShapRFECV.fit_compute" class="md-nav__link">
    <span class="md-ellipsis">
      fit_compute
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#probatus.feature_elimination.feature_elimination.ShapRFECV.get_reduced_features_set" class="md-nav__link">
    <span class="md-ellipsis">
      get_reduced_features_set
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#probatus.feature_elimination.feature_elimination.ShapRFECV.plot" class="md-nav__link">
    <span class="md-ellipsis">
      plot
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


<h1 id="features-elimination">Features Elimination</h1>
<p>This module focuses on feature elimination and it contains two classes:</p>
<ul>
<li><a class="autorefs autorefs-internal" href="#probatus.feature_elimination.feature_elimination.ShapRFECV">ShapRFECV</a>: Perform Backwards Recursive Feature Elimination, using SHAP feature importance. It supports binary classification, regression models and hyperparameter optimization at every feature elimination step. Also for LGBM, XGBoost and CatBoost it support early stopping of the model fitting process. It can be an alternative regularization technique to hyperparameter optimization of the number of base trees in gradient boosted tree models. Particularly useful when dealing with large datasets.</li>
</ul>


<div class="doc doc-object doc-module">



<a id="probatus.feature_elimination.feature_elimination"></a>
    <div class="doc doc-contents first">



  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h2 id="probatus.feature_elimination.feature_elimination.ShapRFECV" class="doc doc-heading">
            <code>ShapRFECV</code>


</h2>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="probatus.utils.BaseFitComputePlotClass">BaseFitComputePlotClass</span></code></p>


      <p>This class performs Backwards Recursive Feature Elimination, using SHAP feature importance.</p>
<p>At each round, for a
    given feature set, starting from all available features, the following steps are applied:</p>
<ol>
<li>(Optional) Tune the hyperparameters of the model using sklearn compatible search CV e.g.
    <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html">GridSearchCV</a>,
    <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html?highlight=randomized#sklearn.model_selection.RandomizedSearchCV">RandomizedSearchCV</a>, or
    <a href="https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html">BayesSearchCV</a>,</li>
<li>Apply Cross-validation (CV) to estimate the SHAP feature importance on the provided dataset. In each CV
    iteration, the model is fitted on the train folds, and applied on the validation fold to estimate
    SHAP feature importance.</li>
<li>Remove <code>step</code> lowest SHAP importance features from the dataset.</li>
</ol>
<p>At the end of the process, the user can plot the performance of the model for each iteration, and select the
    optimal number of features and the features set.</p>
<p>The functionality is
    similar to <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html">RFECV</a>.
    The main difference is removing the lowest importance features based on SHAP features importance. It also
    supports the use of sklearn compatible search CV for hyperparameter optimization e.g.
    <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html">GridSearchCV</a>,
    <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html?highlight=randomized#sklearn.model_selection.RandomizedSearchCV">RandomizedSearchCV</a>, or
    <a href="https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html">BayesSearchCV</a>, which
    needs to be passed as the <code>model</code>. Thanks to this you can perform hyperparameter optimization at each step of
    the feature elimination. Lastly, it supports categorical features (object and category dtype) and missing values
    in the data, as long as the model supports them.</p>
<p>We recommend using <a href="https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html">LGBMClassifier</a>,
    because by default it handles missing values and categorical features. In case of other models, make sure to
    handle these issues for your dataset and consider impact it might have on features importance.</p>
<p>Example:</p>
<pre><code class="language-python">import numpy as np
import pandas as pd
from probatus.feature_elimination import ShapRFECV
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV

feature_names = [
    'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7',
    'f8', 'f9', 'f10', 'f11', 'f12', 'f13',
    'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20']

# Prepare two samples
X, y = make_classification(n_samples=200, class_sep=0.05, n_informative=6, n_features=20,
                           random_state=0, n_redundant=10, n_clusters_per_class=1)
X = pd.DataFrame(X, columns=feature_names)


# Prepare model and parameter search space
model = RandomForestClassifier(max_depth=5, class_weight='balanced')

param_grid = {
    'n_estimators': [5, 7, 10],
    'min_samples_leaf': [3, 5, 7, 10],
}
search = RandomizedSearchCV(model, param_grid)


# Run feature elimination
shap_elimination = ShapRFECV(
    model=search, step=0.2, cv=10, scoring='roc_auc', n_jobs=3)
report = shap_elimination.fit_compute(X, y)

# Make plots
performance_plot = shap_elimination.plot()

# Get final feature set
final_features_set = shap_elimination.get_reduced_features_set(num_features=3)
</code></pre>
<p><img src="../img/shaprfecv.png" width="500" /></p>

              <details class="quote">
                <summary>Source code in <code>probatus/feature_elimination/feature_elimination.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">  23</span>
<span class="normal">  24</span>
<span class="normal">  25</span>
<span class="normal">  26</span>
<span class="normal">  27</span>
<span class="normal">  28</span>
<span class="normal">  29</span>
<span class="normal">  30</span>
<span class="normal">  31</span>
<span class="normal">  32</span>
<span class="normal">  33</span>
<span class="normal">  34</span>
<span class="normal">  35</span>
<span class="normal">  36</span>
<span class="normal">  37</span>
<span class="normal">  38</span>
<span class="normal">  39</span>
<span class="normal">  40</span>
<span class="normal">  41</span>
<span class="normal">  42</span>
<span class="normal">  43</span>
<span class="normal">  44</span>
<span class="normal">  45</span>
<span class="normal">  46</span>
<span class="normal">  47</span>
<span class="normal">  48</span>
<span class="normal">  49</span>
<span class="normal">  50</span>
<span class="normal">  51</span>
<span class="normal">  52</span>
<span class="normal">  53</span>
<span class="normal">  54</span>
<span class="normal">  55</span>
<span class="normal">  56</span>
<span class="normal">  57</span>
<span class="normal">  58</span>
<span class="normal">  59</span>
<span class="normal">  60</span>
<span class="normal">  61</span>
<span class="normal">  62</span>
<span class="normal">  63</span>
<span class="normal">  64</span>
<span class="normal">  65</span>
<span class="normal">  66</span>
<span class="normal">  67</span>
<span class="normal">  68</span>
<span class="normal">  69</span>
<span class="normal">  70</span>
<span class="normal">  71</span>
<span class="normal">  72</span>
<span class="normal">  73</span>
<span class="normal">  74</span>
<span class="normal">  75</span>
<span class="normal">  76</span>
<span class="normal">  77</span>
<span class="normal">  78</span>
<span class="normal">  79</span>
<span class="normal">  80</span>
<span class="normal">  81</span>
<span class="normal">  82</span>
<span class="normal">  83</span>
<span class="normal">  84</span>
<span class="normal">  85</span>
<span class="normal">  86</span>
<span class="normal">  87</span>
<span class="normal">  88</span>
<span class="normal">  89</span>
<span class="normal">  90</span>
<span class="normal">  91</span>
<span class="normal">  92</span>
<span class="normal">  93</span>
<span class="normal">  94</span>
<span class="normal">  95</span>
<span class="normal">  96</span>
<span class="normal">  97</span>
<span class="normal">  98</span>
<span class="normal">  99</span>
<span class="normal"> 100</span>
<span class="normal"> 101</span>
<span class="normal"> 102</span>
<span class="normal"> 103</span>
<span class="normal"> 104</span>
<span class="normal"> 105</span>
<span class="normal"> 106</span>
<span class="normal"> 107</span>
<span class="normal"> 108</span>
<span class="normal"> 109</span>
<span class="normal"> 110</span>
<span class="normal"> 111</span>
<span class="normal"> 112</span>
<span class="normal"> 113</span>
<span class="normal"> 114</span>
<span class="normal"> 115</span>
<span class="normal"> 116</span>
<span class="normal"> 117</span>
<span class="normal"> 118</span>
<span class="normal"> 119</span>
<span class="normal"> 120</span>
<span class="normal"> 121</span>
<span class="normal"> 122</span>
<span class="normal"> 123</span>
<span class="normal"> 124</span>
<span class="normal"> 125</span>
<span class="normal"> 126</span>
<span class="normal"> 127</span>
<span class="normal"> 128</span>
<span class="normal"> 129</span>
<span class="normal"> 130</span>
<span class="normal"> 131</span>
<span class="normal"> 132</span>
<span class="normal"> 133</span>
<span class="normal"> 134</span>
<span class="normal"> 135</span>
<span class="normal"> 136</span>
<span class="normal"> 137</span>
<span class="normal"> 138</span>
<span class="normal"> 139</span>
<span class="normal"> 140</span>
<span class="normal"> 141</span>
<span class="normal"> 142</span>
<span class="normal"> 143</span>
<span class="normal"> 144</span>
<span class="normal"> 145</span>
<span class="normal"> 146</span>
<span class="normal"> 147</span>
<span class="normal"> 148</span>
<span class="normal"> 149</span>
<span class="normal"> 150</span>
<span class="normal"> 151</span>
<span class="normal"> 152</span>
<span class="normal"> 153</span>
<span class="normal"> 154</span>
<span class="normal"> 155</span>
<span class="normal"> 156</span>
<span class="normal"> 157</span>
<span class="normal"> 158</span>
<span class="normal"> 159</span>
<span class="normal"> 160</span>
<span class="normal"> 161</span>
<span class="normal"> 162</span>
<span class="normal"> 163</span>
<span class="normal"> 164</span>
<span class="normal"> 165</span>
<span class="normal"> 166</span>
<span class="normal"> 167</span>
<span class="normal"> 168</span>
<span class="normal"> 169</span>
<span class="normal"> 170</span>
<span class="normal"> 171</span>
<span class="normal"> 172</span>
<span class="normal"> 173</span>
<span class="normal"> 174</span>
<span class="normal"> 175</span>
<span class="normal"> 176</span>
<span class="normal"> 177</span>
<span class="normal"> 178</span>
<span class="normal"> 179</span>
<span class="normal"> 180</span>
<span class="normal"> 181</span>
<span class="normal"> 182</span>
<span class="normal"> 183</span>
<span class="normal"> 184</span>
<span class="normal"> 185</span>
<span class="normal"> 186</span>
<span class="normal"> 187</span>
<span class="normal"> 188</span>
<span class="normal"> 189</span>
<span class="normal"> 190</span>
<span class="normal"> 191</span>
<span class="normal"> 192</span>
<span class="normal"> 193</span>
<span class="normal"> 194</span>
<span class="normal"> 195</span>
<span class="normal"> 196</span>
<span class="normal"> 197</span>
<span class="normal"> 198</span>
<span class="normal"> 199</span>
<span class="normal"> 200</span>
<span class="normal"> 201</span>
<span class="normal"> 202</span>
<span class="normal"> 203</span>
<span class="normal"> 204</span>
<span class="normal"> 205</span>
<span class="normal"> 206</span>
<span class="normal"> 207</span>
<span class="normal"> 208</span>
<span class="normal"> 209</span>
<span class="normal"> 210</span>
<span class="normal"> 211</span>
<span class="normal"> 212</span>
<span class="normal"> 213</span>
<span class="normal"> 214</span>
<span class="normal"> 215</span>
<span class="normal"> 216</span>
<span class="normal"> 217</span>
<span class="normal"> 218</span>
<span class="normal"> 219</span>
<span class="normal"> 220</span>
<span class="normal"> 221</span>
<span class="normal"> 222</span>
<span class="normal"> 223</span>
<span class="normal"> 224</span>
<span class="normal"> 225</span>
<span class="normal"> 226</span>
<span class="normal"> 227</span>
<span class="normal"> 228</span>
<span class="normal"> 229</span>
<span class="normal"> 230</span>
<span class="normal"> 231</span>
<span class="normal"> 232</span>
<span class="normal"> 233</span>
<span class="normal"> 234</span>
<span class="normal"> 235</span>
<span class="normal"> 236</span>
<span class="normal"> 237</span>
<span class="normal"> 238</span>
<span class="normal"> 239</span>
<span class="normal"> 240</span>
<span class="normal"> 241</span>
<span class="normal"> 242</span>
<span class="normal"> 243</span>
<span class="normal"> 244</span>
<span class="normal"> 245</span>
<span class="normal"> 246</span>
<span class="normal"> 247</span>
<span class="normal"> 248</span>
<span class="normal"> 249</span>
<span class="normal"> 250</span>
<span class="normal"> 251</span>
<span class="normal"> 252</span>
<span class="normal"> 253</span>
<span class="normal"> 254</span>
<span class="normal"> 255</span>
<span class="normal"> 256</span>
<span class="normal"> 257</span>
<span class="normal"> 258</span>
<span class="normal"> 259</span>
<span class="normal"> 260</span>
<span class="normal"> 261</span>
<span class="normal"> 262</span>
<span class="normal"> 263</span>
<span class="normal"> 264</span>
<span class="normal"> 265</span>
<span class="normal"> 266</span>
<span class="normal"> 267</span>
<span class="normal"> 268</span>
<span class="normal"> 269</span>
<span class="normal"> 270</span>
<span class="normal"> 271</span>
<span class="normal"> 272</span>
<span class="normal"> 273</span>
<span class="normal"> 274</span>
<span class="normal"> 275</span>
<span class="normal"> 276</span>
<span class="normal"> 277</span>
<span class="normal"> 278</span>
<span class="normal"> 279</span>
<span class="normal"> 280</span>
<span class="normal"> 281</span>
<span class="normal"> 282</span>
<span class="normal"> 283</span>
<span class="normal"> 284</span>
<span class="normal"> 285</span>
<span class="normal"> 286</span>
<span class="normal"> 287</span>
<span class="normal"> 288</span>
<span class="normal"> 289</span>
<span class="normal"> 290</span>
<span class="normal"> 291</span>
<span class="normal"> 292</span>
<span class="normal"> 293</span>
<span class="normal"> 294</span>
<span class="normal"> 295</span>
<span class="normal"> 296</span>
<span class="normal"> 297</span>
<span class="normal"> 298</span>
<span class="normal"> 299</span>
<span class="normal"> 300</span>
<span class="normal"> 301</span>
<span class="normal"> 302</span>
<span class="normal"> 303</span>
<span class="normal"> 304</span>
<span class="normal"> 305</span>
<span class="normal"> 306</span>
<span class="normal"> 307</span>
<span class="normal"> 308</span>
<span class="normal"> 309</span>
<span class="normal"> 310</span>
<span class="normal"> 311</span>
<span class="normal"> 312</span>
<span class="normal"> 313</span>
<span class="normal"> 314</span>
<span class="normal"> 315</span>
<span class="normal"> 316</span>
<span class="normal"> 317</span>
<span class="normal"> 318</span>
<span class="normal"> 319</span>
<span class="normal"> 320</span>
<span class="normal"> 321</span>
<span class="normal"> 322</span>
<span class="normal"> 323</span>
<span class="normal"> 324</span>
<span class="normal"> 325</span>
<span class="normal"> 326</span>
<span class="normal"> 327</span>
<span class="normal"> 328</span>
<span class="normal"> 329</span>
<span class="normal"> 330</span>
<span class="normal"> 331</span>
<span class="normal"> 332</span>
<span class="normal"> 333</span>
<span class="normal"> 334</span>
<span class="normal"> 335</span>
<span class="normal"> 336</span>
<span class="normal"> 337</span>
<span class="normal"> 338</span>
<span class="normal"> 339</span>
<span class="normal"> 340</span>
<span class="normal"> 341</span>
<span class="normal"> 342</span>
<span class="normal"> 343</span>
<span class="normal"> 344</span>
<span class="normal"> 345</span>
<span class="normal"> 346</span>
<span class="normal"> 347</span>
<span class="normal"> 348</span>
<span class="normal"> 349</span>
<span class="normal"> 350</span>
<span class="normal"> 351</span>
<span class="normal"> 352</span>
<span class="normal"> 353</span>
<span class="normal"> 354</span>
<span class="normal"> 355</span>
<span class="normal"> 356</span>
<span class="normal"> 357</span>
<span class="normal"> 358</span>
<span class="normal"> 359</span>
<span class="normal"> 360</span>
<span class="normal"> 361</span>
<span class="normal"> 362</span>
<span class="normal"> 363</span>
<span class="normal"> 364</span>
<span class="normal"> 365</span>
<span class="normal"> 366</span>
<span class="normal"> 367</span>
<span class="normal"> 368</span>
<span class="normal"> 369</span>
<span class="normal"> 370</span>
<span class="normal"> 371</span>
<span class="normal"> 372</span>
<span class="normal"> 373</span>
<span class="normal"> 374</span>
<span class="normal"> 375</span>
<span class="normal"> 376</span>
<span class="normal"> 377</span>
<span class="normal"> 378</span>
<span class="normal"> 379</span>
<span class="normal"> 380</span>
<span class="normal"> 381</span>
<span class="normal"> 382</span>
<span class="normal"> 383</span>
<span class="normal"> 384</span>
<span class="normal"> 385</span>
<span class="normal"> 386</span>
<span class="normal"> 387</span>
<span class="normal"> 388</span>
<span class="normal"> 389</span>
<span class="normal"> 390</span>
<span class="normal"> 391</span>
<span class="normal"> 392</span>
<span class="normal"> 393</span>
<span class="normal"> 394</span>
<span class="normal"> 395</span>
<span class="normal"> 396</span>
<span class="normal"> 397</span>
<span class="normal"> 398</span>
<span class="normal"> 399</span>
<span class="normal"> 400</span>
<span class="normal"> 401</span>
<span class="normal"> 402</span>
<span class="normal"> 403</span>
<span class="normal"> 404</span>
<span class="normal"> 405</span>
<span class="normal"> 406</span>
<span class="normal"> 407</span>
<span class="normal"> 408</span>
<span class="normal"> 409</span>
<span class="normal"> 410</span>
<span class="normal"> 411</span>
<span class="normal"> 412</span>
<span class="normal"> 413</span>
<span class="normal"> 414</span>
<span class="normal"> 415</span>
<span class="normal"> 416</span>
<span class="normal"> 417</span>
<span class="normal"> 418</span>
<span class="normal"> 419</span>
<span class="normal"> 420</span>
<span class="normal"> 421</span>
<span class="normal"> 422</span>
<span class="normal"> 423</span>
<span class="normal"> 424</span>
<span class="normal"> 425</span>
<span class="normal"> 426</span>
<span class="normal"> 427</span>
<span class="normal"> 428</span>
<span class="normal"> 429</span>
<span class="normal"> 430</span>
<span class="normal"> 431</span>
<span class="normal"> 432</span>
<span class="normal"> 433</span>
<span class="normal"> 434</span>
<span class="normal"> 435</span>
<span class="normal"> 436</span>
<span class="normal"> 437</span>
<span class="normal"> 438</span>
<span class="normal"> 439</span>
<span class="normal"> 440</span>
<span class="normal"> 441</span>
<span class="normal"> 442</span>
<span class="normal"> 443</span>
<span class="normal"> 444</span>
<span class="normal"> 445</span>
<span class="normal"> 446</span>
<span class="normal"> 447</span>
<span class="normal"> 448</span>
<span class="normal"> 449</span>
<span class="normal"> 450</span>
<span class="normal"> 451</span>
<span class="normal"> 452</span>
<span class="normal"> 453</span>
<span class="normal"> 454</span>
<span class="normal"> 455</span>
<span class="normal"> 456</span>
<span class="normal"> 457</span>
<span class="normal"> 458</span>
<span class="normal"> 459</span>
<span class="normal"> 460</span>
<span class="normal"> 461</span>
<span class="normal"> 462</span>
<span class="normal"> 463</span>
<span class="normal"> 464</span>
<span class="normal"> 465</span>
<span class="normal"> 466</span>
<span class="normal"> 467</span>
<span class="normal"> 468</span>
<span class="normal"> 469</span>
<span class="normal"> 470</span>
<span class="normal"> 471</span>
<span class="normal"> 472</span>
<span class="normal"> 473</span>
<span class="normal"> 474</span>
<span class="normal"> 475</span>
<span class="normal"> 476</span>
<span class="normal"> 477</span>
<span class="normal"> 478</span>
<span class="normal"> 479</span>
<span class="normal"> 480</span>
<span class="normal"> 481</span>
<span class="normal"> 482</span>
<span class="normal"> 483</span>
<span class="normal"> 484</span>
<span class="normal"> 485</span>
<span class="normal"> 486</span>
<span class="normal"> 487</span>
<span class="normal"> 488</span>
<span class="normal"> 489</span>
<span class="normal"> 490</span>
<span class="normal"> 491</span>
<span class="normal"> 492</span>
<span class="normal"> 493</span>
<span class="normal"> 494</span>
<span class="normal"> 495</span>
<span class="normal"> 496</span>
<span class="normal"> 497</span>
<span class="normal"> 498</span>
<span class="normal"> 499</span>
<span class="normal"> 500</span>
<span class="normal"> 501</span>
<span class="normal"> 502</span>
<span class="normal"> 503</span>
<span class="normal"> 504</span>
<span class="normal"> 505</span>
<span class="normal"> 506</span>
<span class="normal"> 507</span>
<span class="normal"> 508</span>
<span class="normal"> 509</span>
<span class="normal"> 510</span>
<span class="normal"> 511</span>
<span class="normal"> 512</span>
<span class="normal"> 513</span>
<span class="normal"> 514</span>
<span class="normal"> 515</span>
<span class="normal"> 516</span>
<span class="normal"> 517</span>
<span class="normal"> 518</span>
<span class="normal"> 519</span>
<span class="normal"> 520</span>
<span class="normal"> 521</span>
<span class="normal"> 522</span>
<span class="normal"> 523</span>
<span class="normal"> 524</span>
<span class="normal"> 525</span>
<span class="normal"> 526</span>
<span class="normal"> 527</span>
<span class="normal"> 528</span>
<span class="normal"> 529</span>
<span class="normal"> 530</span>
<span class="normal"> 531</span>
<span class="normal"> 532</span>
<span class="normal"> 533</span>
<span class="normal"> 534</span>
<span class="normal"> 535</span>
<span class="normal"> 536</span>
<span class="normal"> 537</span>
<span class="normal"> 538</span>
<span class="normal"> 539</span>
<span class="normal"> 540</span>
<span class="normal"> 541</span>
<span class="normal"> 542</span>
<span class="normal"> 543</span>
<span class="normal"> 544</span>
<span class="normal"> 545</span>
<span class="normal"> 546</span>
<span class="normal"> 547</span>
<span class="normal"> 548</span>
<span class="normal"> 549</span>
<span class="normal"> 550</span>
<span class="normal"> 551</span>
<span class="normal"> 552</span>
<span class="normal"> 553</span>
<span class="normal"> 554</span>
<span class="normal"> 555</span>
<span class="normal"> 556</span>
<span class="normal"> 557</span>
<span class="normal"> 558</span>
<span class="normal"> 559</span>
<span class="normal"> 560</span>
<span class="normal"> 561</span>
<span class="normal"> 562</span>
<span class="normal"> 563</span>
<span class="normal"> 564</span>
<span class="normal"> 565</span>
<span class="normal"> 566</span>
<span class="normal"> 567</span>
<span class="normal"> 568</span>
<span class="normal"> 569</span>
<span class="normal"> 570</span>
<span class="normal"> 571</span>
<span class="normal"> 572</span>
<span class="normal"> 573</span>
<span class="normal"> 574</span>
<span class="normal"> 575</span>
<span class="normal"> 576</span>
<span class="normal"> 577</span>
<span class="normal"> 578</span>
<span class="normal"> 579</span>
<span class="normal"> 580</span>
<span class="normal"> 581</span>
<span class="normal"> 582</span>
<span class="normal"> 583</span>
<span class="normal"> 584</span>
<span class="normal"> 585</span>
<span class="normal"> 586</span>
<span class="normal"> 587</span>
<span class="normal"> 588</span>
<span class="normal"> 589</span>
<span class="normal"> 590</span>
<span class="normal"> 591</span>
<span class="normal"> 592</span>
<span class="normal"> 593</span>
<span class="normal"> 594</span>
<span class="normal"> 595</span>
<span class="normal"> 596</span>
<span class="normal"> 597</span>
<span class="normal"> 598</span>
<span class="normal"> 599</span>
<span class="normal"> 600</span>
<span class="normal"> 601</span>
<span class="normal"> 602</span>
<span class="normal"> 603</span>
<span class="normal"> 604</span>
<span class="normal"> 605</span>
<span class="normal"> 606</span>
<span class="normal"> 607</span>
<span class="normal"> 608</span>
<span class="normal"> 609</span>
<span class="normal"> 610</span>
<span class="normal"> 611</span>
<span class="normal"> 612</span>
<span class="normal"> 613</span>
<span class="normal"> 614</span>
<span class="normal"> 615</span>
<span class="normal"> 616</span>
<span class="normal"> 617</span>
<span class="normal"> 618</span>
<span class="normal"> 619</span>
<span class="normal"> 620</span>
<span class="normal"> 621</span>
<span class="normal"> 622</span>
<span class="normal"> 623</span>
<span class="normal"> 624</span>
<span class="normal"> 625</span>
<span class="normal"> 626</span>
<span class="normal"> 627</span>
<span class="normal"> 628</span>
<span class="normal"> 629</span>
<span class="normal"> 630</span>
<span class="normal"> 631</span>
<span class="normal"> 632</span>
<span class="normal"> 633</span>
<span class="normal"> 634</span>
<span class="normal"> 635</span>
<span class="normal"> 636</span>
<span class="normal"> 637</span>
<span class="normal"> 638</span>
<span class="normal"> 639</span>
<span class="normal"> 640</span>
<span class="normal"> 641</span>
<span class="normal"> 642</span>
<span class="normal"> 643</span>
<span class="normal"> 644</span>
<span class="normal"> 645</span>
<span class="normal"> 646</span>
<span class="normal"> 647</span>
<span class="normal"> 648</span>
<span class="normal"> 649</span>
<span class="normal"> 650</span>
<span class="normal"> 651</span>
<span class="normal"> 652</span>
<span class="normal"> 653</span>
<span class="normal"> 654</span>
<span class="normal"> 655</span>
<span class="normal"> 656</span>
<span class="normal"> 657</span>
<span class="normal"> 658</span>
<span class="normal"> 659</span>
<span class="normal"> 660</span>
<span class="normal"> 661</span>
<span class="normal"> 662</span>
<span class="normal"> 663</span>
<span class="normal"> 664</span>
<span class="normal"> 665</span>
<span class="normal"> 666</span>
<span class="normal"> 667</span>
<span class="normal"> 668</span>
<span class="normal"> 669</span>
<span class="normal"> 670</span>
<span class="normal"> 671</span>
<span class="normal"> 672</span>
<span class="normal"> 673</span>
<span class="normal"> 674</span>
<span class="normal"> 675</span>
<span class="normal"> 676</span>
<span class="normal"> 677</span>
<span class="normal"> 678</span>
<span class="normal"> 679</span>
<span class="normal"> 680</span>
<span class="normal"> 681</span>
<span class="normal"> 682</span>
<span class="normal"> 683</span>
<span class="normal"> 684</span>
<span class="normal"> 685</span>
<span class="normal"> 686</span>
<span class="normal"> 687</span>
<span class="normal"> 688</span>
<span class="normal"> 689</span>
<span class="normal"> 690</span>
<span class="normal"> 691</span>
<span class="normal"> 692</span>
<span class="normal"> 693</span>
<span class="normal"> 694</span>
<span class="normal"> 695</span>
<span class="normal"> 696</span>
<span class="normal"> 697</span>
<span class="normal"> 698</span>
<span class="normal"> 699</span>
<span class="normal"> 700</span>
<span class="normal"> 701</span>
<span class="normal"> 702</span>
<span class="normal"> 703</span>
<span class="normal"> 704</span>
<span class="normal"> 705</span>
<span class="normal"> 706</span>
<span class="normal"> 707</span>
<span class="normal"> 708</span>
<span class="normal"> 709</span>
<span class="normal"> 710</span>
<span class="normal"> 711</span>
<span class="normal"> 712</span>
<span class="normal"> 713</span>
<span class="normal"> 714</span>
<span class="normal"> 715</span>
<span class="normal"> 716</span>
<span class="normal"> 717</span>
<span class="normal"> 718</span>
<span class="normal"> 719</span>
<span class="normal"> 720</span>
<span class="normal"> 721</span>
<span class="normal"> 722</span>
<span class="normal"> 723</span>
<span class="normal"> 724</span>
<span class="normal"> 725</span>
<span class="normal"> 726</span>
<span class="normal"> 727</span>
<span class="normal"> 728</span>
<span class="normal"> 729</span>
<span class="normal"> 730</span>
<span class="normal"> 731</span>
<span class="normal"> 732</span>
<span class="normal"> 733</span>
<span class="normal"> 734</span>
<span class="normal"> 735</span>
<span class="normal"> 736</span>
<span class="normal"> 737</span>
<span class="normal"> 738</span>
<span class="normal"> 739</span>
<span class="normal"> 740</span>
<span class="normal"> 741</span>
<span class="normal"> 742</span>
<span class="normal"> 743</span>
<span class="normal"> 744</span>
<span class="normal"> 745</span>
<span class="normal"> 746</span>
<span class="normal"> 747</span>
<span class="normal"> 748</span>
<span class="normal"> 749</span>
<span class="normal"> 750</span>
<span class="normal"> 751</span>
<span class="normal"> 752</span>
<span class="normal"> 753</span>
<span class="normal"> 754</span>
<span class="normal"> 755</span>
<span class="normal"> 756</span>
<span class="normal"> 757</span>
<span class="normal"> 758</span>
<span class="normal"> 759</span>
<span class="normal"> 760</span>
<span class="normal"> 761</span>
<span class="normal"> 762</span>
<span class="normal"> 763</span>
<span class="normal"> 764</span>
<span class="normal"> 765</span>
<span class="normal"> 766</span>
<span class="normal"> 767</span>
<span class="normal"> 768</span>
<span class="normal"> 769</span>
<span class="normal"> 770</span>
<span class="normal"> 771</span>
<span class="normal"> 772</span>
<span class="normal"> 773</span>
<span class="normal"> 774</span>
<span class="normal"> 775</span>
<span class="normal"> 776</span>
<span class="normal"> 777</span>
<span class="normal"> 778</span>
<span class="normal"> 779</span>
<span class="normal"> 780</span>
<span class="normal"> 781</span>
<span class="normal"> 782</span>
<span class="normal"> 783</span>
<span class="normal"> 784</span>
<span class="normal"> 785</span>
<span class="normal"> 786</span>
<span class="normal"> 787</span>
<span class="normal"> 788</span>
<span class="normal"> 789</span>
<span class="normal"> 790</span>
<span class="normal"> 791</span>
<span class="normal"> 792</span>
<span class="normal"> 793</span>
<span class="normal"> 794</span>
<span class="normal"> 795</span>
<span class="normal"> 796</span>
<span class="normal"> 797</span>
<span class="normal"> 798</span>
<span class="normal"> 799</span>
<span class="normal"> 800</span>
<span class="normal"> 801</span>
<span class="normal"> 802</span>
<span class="normal"> 803</span>
<span class="normal"> 804</span>
<span class="normal"> 805</span>
<span class="normal"> 806</span>
<span class="normal"> 807</span>
<span class="normal"> 808</span>
<span class="normal"> 809</span>
<span class="normal"> 810</span>
<span class="normal"> 811</span>
<span class="normal"> 812</span>
<span class="normal"> 813</span>
<span class="normal"> 814</span>
<span class="normal"> 815</span>
<span class="normal"> 816</span>
<span class="normal"> 817</span>
<span class="normal"> 818</span>
<span class="normal"> 819</span>
<span class="normal"> 820</span>
<span class="normal"> 821</span>
<span class="normal"> 822</span>
<span class="normal"> 823</span>
<span class="normal"> 824</span>
<span class="normal"> 825</span>
<span class="normal"> 826</span>
<span class="normal"> 827</span>
<span class="normal"> 828</span>
<span class="normal"> 829</span>
<span class="normal"> 830</span>
<span class="normal"> 831</span>
<span class="normal"> 832</span>
<span class="normal"> 833</span>
<span class="normal"> 834</span>
<span class="normal"> 835</span>
<span class="normal"> 836</span>
<span class="normal"> 837</span>
<span class="normal"> 838</span>
<span class="normal"> 839</span>
<span class="normal"> 840</span>
<span class="normal"> 841</span>
<span class="normal"> 842</span>
<span class="normal"> 843</span>
<span class="normal"> 844</span>
<span class="normal"> 845</span>
<span class="normal"> 846</span>
<span class="normal"> 847</span>
<span class="normal"> 848</span>
<span class="normal"> 849</span>
<span class="normal"> 850</span>
<span class="normal"> 851</span>
<span class="normal"> 852</span>
<span class="normal"> 853</span>
<span class="normal"> 854</span>
<span class="normal"> 855</span>
<span class="normal"> 856</span>
<span class="normal"> 857</span>
<span class="normal"> 858</span>
<span class="normal"> 859</span>
<span class="normal"> 860</span>
<span class="normal"> 861</span>
<span class="normal"> 862</span>
<span class="normal"> 863</span>
<span class="normal"> 864</span>
<span class="normal"> 865</span>
<span class="normal"> 866</span>
<span class="normal"> 867</span>
<span class="normal"> 868</span>
<span class="normal"> 869</span>
<span class="normal"> 870</span>
<span class="normal"> 871</span>
<span class="normal"> 872</span>
<span class="normal"> 873</span>
<span class="normal"> 874</span>
<span class="normal"> 875</span>
<span class="normal"> 876</span>
<span class="normal"> 877</span>
<span class="normal"> 878</span>
<span class="normal"> 879</span>
<span class="normal"> 880</span>
<span class="normal"> 881</span>
<span class="normal"> 882</span>
<span class="normal"> 883</span>
<span class="normal"> 884</span>
<span class="normal"> 885</span>
<span class="normal"> 886</span>
<span class="normal"> 887</span>
<span class="normal"> 888</span>
<span class="normal"> 889</span>
<span class="normal"> 890</span>
<span class="normal"> 891</span>
<span class="normal"> 892</span>
<span class="normal"> 893</span>
<span class="normal"> 894</span>
<span class="normal"> 895</span>
<span class="normal"> 896</span>
<span class="normal"> 897</span>
<span class="normal"> 898</span>
<span class="normal"> 899</span>
<span class="normal"> 900</span>
<span class="normal"> 901</span>
<span class="normal"> 902</span>
<span class="normal"> 903</span>
<span class="normal"> 904</span>
<span class="normal"> 905</span>
<span class="normal"> 906</span>
<span class="normal"> 907</span>
<span class="normal"> 908</span>
<span class="normal"> 909</span>
<span class="normal"> 910</span>
<span class="normal"> 911</span>
<span class="normal"> 912</span>
<span class="normal"> 913</span>
<span class="normal"> 914</span>
<span class="normal"> 915</span>
<span class="normal"> 916</span>
<span class="normal"> 917</span>
<span class="normal"> 918</span>
<span class="normal"> 919</span>
<span class="normal"> 920</span>
<span class="normal"> 921</span>
<span class="normal"> 922</span>
<span class="normal"> 923</span>
<span class="normal"> 924</span>
<span class="normal"> 925</span>
<span class="normal"> 926</span>
<span class="normal"> 927</span>
<span class="normal"> 928</span>
<span class="normal"> 929</span>
<span class="normal"> 930</span>
<span class="normal"> 931</span>
<span class="normal"> 932</span>
<span class="normal"> 933</span>
<span class="normal"> 934</span>
<span class="normal"> 935</span>
<span class="normal"> 936</span>
<span class="normal"> 937</span>
<span class="normal"> 938</span>
<span class="normal"> 939</span>
<span class="normal"> 940</span>
<span class="normal"> 941</span>
<span class="normal"> 942</span>
<span class="normal"> 943</span>
<span class="normal"> 944</span>
<span class="normal"> 945</span>
<span class="normal"> 946</span>
<span class="normal"> 947</span>
<span class="normal"> 948</span>
<span class="normal"> 949</span>
<span class="normal"> 950</span>
<span class="normal"> 951</span>
<span class="normal"> 952</span>
<span class="normal"> 953</span>
<span class="normal"> 954</span>
<span class="normal"> 955</span>
<span class="normal"> 956</span>
<span class="normal"> 957</span>
<span class="normal"> 958</span>
<span class="normal"> 959</span>
<span class="normal"> 960</span>
<span class="normal"> 961</span>
<span class="normal"> 962</span>
<span class="normal"> 963</span>
<span class="normal"> 964</span>
<span class="normal"> 965</span>
<span class="normal"> 966</span>
<span class="normal"> 967</span>
<span class="normal"> 968</span>
<span class="normal"> 969</span>
<span class="normal"> 970</span>
<span class="normal"> 971</span>
<span class="normal"> 972</span>
<span class="normal"> 973</span>
<span class="normal"> 974</span>
<span class="normal"> 975</span>
<span class="normal"> 976</span>
<span class="normal"> 977</span>
<span class="normal"> 978</span>
<span class="normal"> 979</span>
<span class="normal"> 980</span>
<span class="normal"> 981</span>
<span class="normal"> 982</span>
<span class="normal"> 983</span>
<span class="normal"> 984</span>
<span class="normal"> 985</span>
<span class="normal"> 986</span>
<span class="normal"> 987</span>
<span class="normal"> 988</span>
<span class="normal"> 989</span>
<span class="normal"> 990</span>
<span class="normal"> 991</span>
<span class="normal"> 992</span>
<span class="normal"> 993</span>
<span class="normal"> 994</span>
<span class="normal"> 995</span>
<span class="normal"> 996</span>
<span class="normal"> 997</span>
<span class="normal"> 998</span>
<span class="normal"> 999</span>
<span class="normal">1000</span>
<span class="normal">1001</span>
<span class="normal">1002</span>
<span class="normal">1003</span>
<span class="normal">1004</span>
<span class="normal">1005</span>
<span class="normal">1006</span>
<span class="normal">1007</span>
<span class="normal">1008</span>
<span class="normal">1009</span>
<span class="normal">1010</span>
<span class="normal">1011</span>
<span class="normal">1012</span>
<span class="normal">1013</span>
<span class="normal">1014</span>
<span class="normal">1015</span>
<span class="normal">1016</span>
<span class="normal">1017</span>
<span class="normal">1018</span>
<span class="normal">1019</span>
<span class="normal">1020</span>
<span class="normal">1021</span>
<span class="normal">1022</span>
<span class="normal">1023</span>
<span class="normal">1024</span>
<span class="normal">1025</span>
<span class="normal">1026</span>
<span class="normal">1027</span>
<span class="normal">1028</span>
<span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span>
<span class="normal">1033</span>
<span class="normal">1034</span>
<span class="normal">1035</span>
<span class="normal">1036</span>
<span class="normal">1037</span>
<span class="normal">1038</span>
<span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span>
<span class="normal">1043</span>
<span class="normal">1044</span>
<span class="normal">1045</span>
<span class="normal">1046</span>
<span class="normal">1047</span>
<span class="normal">1048</span>
<span class="normal">1049</span>
<span class="normal">1050</span>
<span class="normal">1051</span>
<span class="normal">1052</span>
<span class="normal">1053</span>
<span class="normal">1054</span>
<span class="normal">1055</span>
<span class="normal">1056</span>
<span class="normal">1057</span>
<span class="normal">1058</span>
<span class="normal">1059</span>
<span class="normal">1060</span>
<span class="normal">1061</span>
<span class="normal">1062</span>
<span class="normal">1063</span>
<span class="normal">1064</span>
<span class="normal">1065</span>
<span class="normal">1066</span>
<span class="normal">1067</span>
<span class="normal">1068</span>
<span class="normal">1069</span>
<span class="normal">1070</span>
<span class="normal">1071</span>
<span class="normal">1072</span>
<span class="normal">1073</span>
<span class="normal">1074</span>
<span class="normal">1075</span>
<span class="normal">1076</span>
<span class="normal">1077</span>
<span class="normal">1078</span>
<span class="normal">1079</span>
<span class="normal">1080</span>
<span class="normal">1081</span>
<span class="normal">1082</span>
<span class="normal">1083</span>
<span class="normal">1084</span>
<span class="normal">1085</span>
<span class="normal">1086</span>
<span class="normal">1087</span>
<span class="normal">1088</span>
<span class="normal">1089</span>
<span class="normal">1090</span>
<span class="normal">1091</span>
<span class="normal">1092</span>
<span class="normal">1093</span>
<span class="normal">1094</span>
<span class="normal">1095</span>
<span class="normal">1096</span>
<span class="normal">1097</span>
<span class="normal">1098</span>
<span class="normal">1099</span>
<span class="normal">1100</span>
<span class="normal">1101</span>
<span class="normal">1102</span>
<span class="normal">1103</span>
<span class="normal">1104</span>
<span class="normal">1105</span>
<span class="normal">1106</span>
<span class="normal">1107</span>
<span class="normal">1108</span>
<span class="normal">1109</span>
<span class="normal">1110</span>
<span class="normal">1111</span>
<span class="normal">1112</span>
<span class="normal">1113</span>
<span class="normal">1114</span>
<span class="normal">1115</span>
<span class="normal">1116</span>
<span class="normal">1117</span>
<span class="normal">1118</span>
<span class="normal">1119</span>
<span class="normal">1120</span>
<span class="normal">1121</span>
<span class="normal">1122</span>
<span class="normal">1123</span>
<span class="normal">1124</span>
<span class="normal">1125</span>
<span class="normal">1126</span>
<span class="normal">1127</span>
<span class="normal">1128</span>
<span class="normal">1129</span>
<span class="normal">1130</span>
<span class="normal">1131</span>
<span class="normal">1132</span>
<span class="normal">1133</span>
<span class="normal">1134</span>
<span class="normal">1135</span>
<span class="normal">1136</span>
<span class="normal">1137</span>
<span class="normal">1138</span>
<span class="normal">1139</span>
<span class="normal">1140</span>
<span class="normal">1141</span>
<span class="normal">1142</span>
<span class="normal">1143</span>
<span class="normal">1144</span>
<span class="normal">1145</span>
<span class="normal">1146</span>
<span class="normal">1147</span>
<span class="normal">1148</span>
<span class="normal">1149</span>
<span class="normal">1150</span>
<span class="normal">1151</span>
<span class="normal">1152</span>
<span class="normal">1153</span>
<span class="normal">1154</span>
<span class="normal">1155</span>
<span class="normal">1156</span>
<span class="normal">1157</span>
<span class="normal">1158</span>
<span class="normal">1159</span>
<span class="normal">1160</span>
<span class="normal">1161</span>
<span class="normal">1162</span>
<span class="normal">1163</span>
<span class="normal">1164</span>
<span class="normal">1165</span>
<span class="normal">1166</span>
<span class="normal">1167</span>
<span class="normal">1168</span>
<span class="normal">1169</span>
<span class="normal">1170</span>
<span class="normal">1171</span>
<span class="normal">1172</span>
<span class="normal">1173</span>
<span class="normal">1174</span>
<span class="normal">1175</span>
<span class="normal">1176</span>
<span class="normal">1177</span>
<span class="normal">1178</span>
<span class="normal">1179</span>
<span class="normal">1180</span>
<span class="normal">1181</span>
<span class="normal">1182</span>
<span class="normal">1183</span>
<span class="normal">1184</span>
<span class="normal">1185</span>
<span class="normal">1186</span>
<span class="normal">1187</span>
<span class="normal">1188</span>
<span class="normal">1189</span>
<span class="normal">1190</span>
<span class="normal">1191</span>
<span class="normal">1192</span>
<span class="normal">1193</span>
<span class="normal">1194</span>
<span class="normal">1195</span>
<span class="normal">1196</span>
<span class="normal">1197</span>
<span class="normal">1198</span>
<span class="normal">1199</span>
<span class="normal">1200</span>
<span class="normal">1201</span>
<span class="normal">1202</span>
<span class="normal">1203</span>
<span class="normal">1204</span>
<span class="normal">1205</span>
<span class="normal">1206</span>
<span class="normal">1207</span>
<span class="normal">1208</span>
<span class="normal">1209</span>
<span class="normal">1210</span>
<span class="normal">1211</span>
<span class="normal">1212</span>
<span class="normal">1213</span>
<span class="normal">1214</span>
<span class="normal">1215</span>
<span class="normal">1216</span>
<span class="normal">1217</span>
<span class="normal">1218</span>
<span class="normal">1219</span>
<span class="normal">1220</span>
<span class="normal">1221</span>
<span class="normal">1222</span>
<span class="normal">1223</span>
<span class="normal">1224</span>
<span class="normal">1225</span>
<span class="normal">1226</span>
<span class="normal">1227</span>
<span class="normal">1228</span>
<span class="normal">1229</span>
<span class="normal">1230</span>
<span class="normal">1231</span>
<span class="normal">1232</span>
<span class="normal">1233</span>
<span class="normal">1234</span>
<span class="normal">1235</span>
<span class="normal">1236</span>
<span class="normal">1237</span>
<span class="normal">1238</span>
<span class="normal">1239</span>
<span class="normal">1240</span>
<span class="normal">1241</span>
<span class="normal">1242</span>
<span class="normal">1243</span>
<span class="normal">1244</span>
<span class="normal">1245</span>
<span class="normal">1246</span>
<span class="normal">1247</span>
<span class="normal">1248</span>
<span class="normal">1249</span>
<span class="normal">1250</span>
<span class="normal">1251</span>
<span class="normal">1252</span>
<span class="normal">1253</span>
<span class="normal">1254</span>
<span class="normal">1255</span>
<span class="normal">1256</span>
<span class="normal">1257</span>
<span class="normal">1258</span>
<span class="normal">1259</span>
<span class="normal">1260</span>
<span class="normal">1261</span>
<span class="normal">1262</span>
<span class="normal">1263</span>
<span class="normal">1264</span>
<span class="normal">1265</span>
<span class="normal">1266</span>
<span class="normal">1267</span>
<span class="normal">1268</span>
<span class="normal">1269</span>
<span class="normal">1270</span>
<span class="normal">1271</span>
<span class="normal">1272</span>
<span class="normal">1273</span>
<span class="normal">1274</span>
<span class="normal">1275</span>
<span class="normal">1276</span>
<span class="normal">1277</span>
<span class="normal">1278</span>
<span class="normal">1279</span>
<span class="normal">1280</span>
<span class="normal">1281</span>
<span class="normal">1282</span>
<span class="normal">1283</span>
<span class="normal">1284</span>
<span class="normal">1285</span>
<span class="normal">1286</span>
<span class="normal">1287</span>
<span class="normal">1288</span>
<span class="normal">1289</span>
<span class="normal">1290</span>
<span class="normal">1291</span>
<span class="normal">1292</span>
<span class="normal">1293</span>
<span class="normal">1294</span>
<span class="normal">1295</span>
<span class="normal">1296</span>
<span class="normal">1297</span>
<span class="normal">1298</span>
<span class="normal">1299</span>
<span class="normal">1300</span>
<span class="normal">1301</span>
<span class="normal">1302</span>
<span class="normal">1303</span>
<span class="normal">1304</span>
<span class="normal">1305</span>
<span class="normal">1306</span>
<span class="normal">1307</span>
<span class="normal">1308</span>
<span class="normal">1309</span>
<span class="normal">1310</span>
<span class="normal">1311</span>
<span class="normal">1312</span>
<span class="normal">1313</span>
<span class="normal">1314</span>
<span class="normal">1315</span>
<span class="normal">1316</span>
<span class="normal">1317</span>
<span class="normal">1318</span>
<span class="normal">1319</span>
<span class="normal">1320</span>
<span class="normal">1321</span>
<span class="normal">1322</span>
<span class="normal">1323</span>
<span class="normal">1324</span>
<span class="normal">1325</span>
<span class="normal">1326</span>
<span class="normal">1327</span>
<span class="normal">1328</span>
<span class="normal">1329</span>
<span class="normal">1330</span>
<span class="normal">1331</span>
<span class="normal">1332</span>
<span class="normal">1333</span>
<span class="normal">1334</span>
<span class="normal">1335</span>
<span class="normal">1336</span>
<span class="normal">1337</span>
<span class="normal">1338</span>
<span class="normal">1339</span>
<span class="normal">1340</span>
<span class="normal">1341</span>
<span class="normal">1342</span>
<span class="normal">1343</span>
<span class="normal">1344</span>
<span class="normal">1345</span>
<span class="normal">1346</span>
<span class="normal">1347</span>
<span class="normal">1348</span>
<span class="normal">1349</span>
<span class="normal">1350</span>
<span class="normal">1351</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">ShapRFECV</span><span class="p">(</span><span class="n">BaseFitComputePlotClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This class performs Backwards Recursive Feature Elimination, using SHAP feature importance.</span>

<span class="sd">    At each round, for a</span>
<span class="sd">        given feature set, starting from all available features, the following steps are applied:</span>

<span class="sd">    1. (Optional) Tune the hyperparameters of the model using sklearn compatible search CV e.g.</span>
<span class="sd">        [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html),</span>
<span class="sd">        [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html?highlight=randomized#sklearn.model_selection.RandomizedSearchCV), or</span>
<span class="sd">        [BayesSearchCV](https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html),</span>
<span class="sd">    2. Apply Cross-validation (CV) to estimate the SHAP feature importance on the provided dataset. In each CV</span>
<span class="sd">        iteration, the model is fitted on the train folds, and applied on the validation fold to estimate</span>
<span class="sd">        SHAP feature importance.</span>
<span class="sd">    3. Remove `step` lowest SHAP importance features from the dataset.</span>

<span class="sd">    At the end of the process, the user can plot the performance of the model for each iteration, and select the</span>
<span class="sd">        optimal number of features and the features set.</span>

<span class="sd">    The functionality is</span>
<span class="sd">        similar to [RFECV](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html).</span>
<span class="sd">        The main difference is removing the lowest importance features based on SHAP features importance. It also</span>
<span class="sd">        supports the use of sklearn compatible search CV for hyperparameter optimization e.g.</span>
<span class="sd">        [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html),</span>
<span class="sd">        [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html?highlight=randomized#sklearn.model_selection.RandomizedSearchCV), or</span>
<span class="sd">        [BayesSearchCV](https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html), which</span>
<span class="sd">        needs to be passed as the `model`. Thanks to this you can perform hyperparameter optimization at each step of</span>
<span class="sd">        the feature elimination. Lastly, it supports categorical features (object and category dtype) and missing values</span>
<span class="sd">        in the data, as long as the model supports them.</span>

<span class="sd">    We recommend using [LGBMClassifier](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html),</span>
<span class="sd">        because by default it handles missing values and categorical features. In case of other models, make sure to</span>
<span class="sd">        handle these issues for your dataset and consider impact it might have on features importance.</span>


<span class="sd">    Example:</span>
<span class="sd">    ```python</span>
<span class="sd">    import numpy as np</span>
<span class="sd">    import pandas as pd</span>
<span class="sd">    from probatus.feature_elimination import ShapRFECV</span>
<span class="sd">    from sklearn.datasets import make_classification</span>
<span class="sd">    from sklearn.model_selection import train_test_split</span>
<span class="sd">    from sklearn.ensemble import RandomForestClassifier</span>
<span class="sd">    from sklearn.model_selection import RandomizedSearchCV</span>

<span class="sd">    feature_names = [</span>
<span class="sd">        &#39;f1&#39;, &#39;f2&#39;, &#39;f3&#39;, &#39;f4&#39;, &#39;f5&#39;, &#39;f6&#39;, &#39;f7&#39;,</span>
<span class="sd">        &#39;f8&#39;, &#39;f9&#39;, &#39;f10&#39;, &#39;f11&#39;, &#39;f12&#39;, &#39;f13&#39;,</span>
<span class="sd">        &#39;f14&#39;, &#39;f15&#39;, &#39;f16&#39;, &#39;f17&#39;, &#39;f18&#39;, &#39;f19&#39;, &#39;f20&#39;]</span>

<span class="sd">    # Prepare two samples</span>
<span class="sd">    X, y = make_classification(n_samples=200, class_sep=0.05, n_informative=6, n_features=20,</span>
<span class="sd">                               random_state=0, n_redundant=10, n_clusters_per_class=1)</span>
<span class="sd">    X = pd.DataFrame(X, columns=feature_names)</span>


<span class="sd">    # Prepare model and parameter search space</span>
<span class="sd">    model = RandomForestClassifier(max_depth=5, class_weight=&#39;balanced&#39;)</span>

<span class="sd">    param_grid = {</span>
<span class="sd">        &#39;n_estimators&#39;: [5, 7, 10],</span>
<span class="sd">        &#39;min_samples_leaf&#39;: [3, 5, 7, 10],</span>
<span class="sd">    }</span>
<span class="sd">    search = RandomizedSearchCV(model, param_grid)</span>


<span class="sd">    # Run feature elimination</span>
<span class="sd">    shap_elimination = ShapRFECV(</span>
<span class="sd">        model=search, step=0.2, cv=10, scoring=&#39;roc_auc&#39;, n_jobs=3)</span>
<span class="sd">    report = shap_elimination.fit_compute(X, y)</span>

<span class="sd">    # Make plots</span>
<span class="sd">    performance_plot = shap_elimination.plot()</span>

<span class="sd">    # Get final feature set</span>
<span class="sd">    final_features_set = shap_elimination.get_reduced_features_set(num_features=3)</span>
<span class="sd">    ```</span>
<span class="sd">    &lt;img src=&quot;../img/shaprfecv.png&quot; width=&quot;500&quot; /&gt;</span>

<span class="sd">    &quot;&quot;&quot;</span>  <span class="c1"># noqa</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">min_features_to_select</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">cv</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">scoring</span><span class="o">=</span><span class="s2">&quot;roc_auc&quot;</span><span class="p">,</span>
        <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">eval_metric</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method initializes the class.</span>

<span class="sd">        Args:</span>
<span class="sd">            model (classifier or regressor, sklearn compatible search CV e.g. GridSearchCV, RandomizedSearchCV or BayesSearchCV):</span>
<span class="sd">                A model that will be optimized and trained at each round of feature elimination. The recommended model</span>
<span class="sd">                is [LGBMClassifier](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html),</span>
<span class="sd">                because it by default handles the missing values and categorical variables. This parameter also supports</span>
<span class="sd">                any hyperparameter search schema that is consistent with the sklearn API e.g.</span>
<span class="sd">                [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html),</span>
<span class="sd">                [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)</span>
<span class="sd">                or [BayesSearchCV](https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html#skopt.BayesSearchCV).</span>

<span class="sd">            step (int or float, optional):</span>
<span class="sd">                Number of lowest importance features removed each round. If it is an int, then each round such a number of</span>
<span class="sd">                features are discarded. If float, such a percentage of remaining features (rounded down) is removed each</span>
<span class="sd">                iteration. It is recommended to use float, since it is faster for a large number of features, and slows</span>
<span class="sd">                down and becomes more precise with fewer features. Note: the last round may remove fewer features in</span>
<span class="sd">                order to reach min_features_to_select.</span>
<span class="sd">                If columns_to_keep parameter is specified in the fit method, step is the number of features to remove after</span>
<span class="sd">                keeping those columns.</span>

<span class="sd">            min_features_to_select (int, optional):</span>
<span class="sd">                Minimum number of features to be kept. This is a stopping criterion of the feature elimination. By</span>
<span class="sd">                default the process stops when one feature is left. If columns_to_keep is specified in the fit method,</span>
<span class="sd">                it may override this parameter to the maximum between length of columns_to_keep the two.</span>

<span class="sd">            cv (int, cross-validation generator or an iterable, optional):</span>
<span class="sd">                Determines the cross-validation splitting strategy. Compatible with sklearn</span>
<span class="sd">                [cv parameter](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html).</span>
<span class="sd">                If None, then cv of 5 is used.</span>

<span class="sd">            scoring (string or probatus.utils.Scorer, optional):</span>
<span class="sd">                Metric for which the model performance is calculated. It can be either a metric name aligned with predefined</span>
<span class="sd">                [classification scorers names in sklearn](https://scikit-learn.org/stable/modules/model_evaluation.html).</span>

<span class="sd">            n_jobs (int, optional):</span>
<span class="sd">                Number of cores to run in parallel while fitting across folds. None means 1 unless in a</span>
<span class="sd">                `joblib.parallel_backend` context. -1 means using all processors.</span>

<span class="sd">            verbose (int, optional):</span>
<span class="sd">                Controls verbosity of the output:</span>

<span class="sd">                - 0 - neither prints nor warnings are shown</span>
<span class="sd">                - 1 - only most important warnings</span>
<span class="sd">                - 2 - shows all prints and all warnings.</span>

<span class="sd">            random_state (int, optional):</span>
<span class="sd">                Random state set at each round of feature elimination. If it is None, the results will not be</span>
<span class="sd">                reproducible and in random search at each iteration a different hyperparameters might be tested. For</span>
<span class="sd">                reproducible results set it to an integer.</span>

<span class="sd">            early_stopping_rounds (int, optional):</span>
<span class="sd">                Number of rounds with constant performance after which the model fitting stops. This is passed to the</span>
<span class="sd">                fit method of the model for Shapley values estimation, but not for hyperparameter search. Only</span>
<span class="sd">                supported by some models, such as XGBoost, LightGBM and CatBoost. Only recommended when dealing with large sets of data.</span>

<span class="sd">            eval_metric (str, optional):</span>
<span class="sd">                Metric for scoring fitting rounds and activating early stopping. This is passed to the</span>
<span class="sd">                fit method of the model for Shapley values estimation, but not for hyperparameter search. Only</span>
<span class="sd">                supported by some models, such as [XGBoost](https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters)</span>
<span class="sd">                and [LightGBM](https://lightgbm.readthedocs.io/en/latest/Parameters.html#metric-parameters).</span>
<span class="sd">                Note that `eval_metric` is an argument of the model&#39;s fit method and it is different from `scoring`.</span>
<span class="sd">                Only recommended when dealing with large sets of data.</span>
<span class="sd">        &quot;&quot;&quot;</span>  <span class="c1"># noqa</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">search_model</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">BaseSearchCV</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_step</span><span class="p">(</span><span class="n">step</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_features_to_select</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_min_features</span><span class="p">(</span><span class="n">min_features_to_select</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cv</span> <span class="o">=</span> <span class="n">cv</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scorer</span> <span class="o">=</span> <span class="n">get_single_scorer</span><span class="p">(</span><span class="n">scoring</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span> <span class="o">=</span> <span class="n">n_jobs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>

        <span class="c1"># Enable early stopping behavior</span>
        <span class="k">if</span> <span class="n">early_stopping_rounds</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">eval_metric</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="s2">&quot;Running early stopping, requires both &#39;early_stopping_rounds&#39; and &#39;eval_metric&#39; as&quot;</span>
                    <span class="s2">&quot; parameters to be provided and supports only &#39;XGBoost&#39;, &#39;LGBM&#39; and &#39;CatBoost&#39;.&quot;</span>
                <span class="p">)</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">early_stopping_rounds</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="n">early_stopping_rounds</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;early_stopping_rounds must be a positive integer; got </span><span class="si">{</span><span class="n">early_stopping_rounds</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_if_model_is_compatible_with_early_stopping</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Only &#39;XGBoost&#39;, &#39;LGBM&#39; and &#39;CatBoost&#39; supported for early stopping.&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">early_stopping_rounds</span> <span class="o">=</span> <span class="n">early_stopping_rounds</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eval_metric</span> <span class="o">=</span> <span class="n">eval_metric</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_check_if_model_is_compatible_with_early_stopping</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Check if the model or the estimator of the cv is compatible with early stopping.</span>

<span class="sd">        Returns:</span>
<span class="sd">            (bool):</span>
<span class="sd">                bool if true or false based on compatibility.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">libraries</span> <span class="o">=</span> <span class="p">[(</span><span class="s2">&quot;lightgbm&quot;</span><span class="p">,</span> <span class="s2">&quot;LGBMModel&quot;</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;xgboost.sklearn&quot;</span><span class="p">,</span> <span class="s2">&quot;XGBModel&quot;</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;catboost&quot;</span><span class="p">,</span> <span class="s2">&quot;CatBoost&quot;</span><span class="p">)]</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">BaseSearchCV</span><span class="p">):</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">estimator</span>

        <span class="k">for</span> <span class="n">lib</span><span class="p">,</span> <span class="n">class_name</span> <span class="ow">in</span> <span class="n">libraries</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">module</span> <span class="o">=</span> <span class="nb">__import__</span><span class="p">(</span><span class="n">lib</span><span class="p">,</span> <span class="n">fromlist</span><span class="o">=</span><span class="p">[</span><span class="n">class_name</span><span class="p">])</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">class_name</span><span class="p">)):</span>
                    <span class="k">return</span> <span class="kc">True</span>
            <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
                <span class="k">pass</span>

        <span class="k">return</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">compute</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Checks if fit() method has been run.</span>

<span class="sd">        and computes the DataFrame with results of feature elimination for each round.</span>

<span class="sd">        Returns:</span>
<span class="sd">            (pd.DataFrame):</span>
<span class="sd">                DataFrame with results of feature elimination for each round.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_if_fitted</span><span class="p">()</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span>

    <span class="k">def</span> <span class="nf">fit_compute</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">X</span><span class="p">,</span>
        <span class="n">y</span><span class="p">,</span>
        <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">columns_to_keep</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">column_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">shap_variance_penalty_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">shap_kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fits the object with the provided data.</span>

<span class="sd">        The algorithm starts with the entire dataset, and then sequentially</span>
<span class="sd">            eliminates features. If sklearn compatible search CV is passed as model e.g.</span>
<span class="sd">            [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html),</span>
<span class="sd">            [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)</span>
<span class="sd">            or [BayesSearchCV](https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html),</span>
<span class="sd">            the hyperparameter optimization is applied at each step of the elimination.</span>
<span class="sd">            Then, the SHAP feature importance is calculated using Cross-Validation,</span>
<span class="sd">            and `step` lowest importance features are removed. At the end, the</span>
<span class="sd">            report containing results from each iteration is computed and returned to the user.</span>

<span class="sd">        Args:</span>
<span class="sd">            X (pd.DataFrame):</span>
<span class="sd">                Provided dataset.</span>

<span class="sd">            y (pd.Series):</span>
<span class="sd">                Labels for X.</span>

<span class="sd">            sample_weight (pd.Series, np.ndarray, list, optional):</span>
<span class="sd">                array-like of shape (n_samples,) - only use if the model you&#39;re using supports</span>
<span class="sd">                sample weighting (check the corresponding scikit-learn documentation).</span>
<span class="sd">                Array of weights that are assigned to individual samples.</span>
<span class="sd">                Note that they&#39;re only used for fitting of  the model, not during evaluation of metrics.</span>
<span class="sd">                If not provided, then each sample is given unit weight.</span>

<span class="sd">            columns_to_keep (list of str, optional):</span>
<span class="sd">                List of columns to keep. If given, these columns will not be eliminated.</span>

<span class="sd">            column_names (list of str, optional):</span>
<span class="sd">                List of feature names of the provided samples. If provided it will be used to overwrite the existing</span>
<span class="sd">                feature names. If not provided the existing feature names are used or default feature names are</span>
<span class="sd">                generated.</span>

<span class="sd">            shap_variance_penalty_factor (int or float, optional):</span>
<span class="sd">                Apply aggregation penalty when computing average of shap values for a given feature.</span>
<span class="sd">                Results in a preference for features that have smaller standard deviation of shap</span>
<span class="sd">                values (more coherent shap importance). Recommend value 0.5 - 1.0.</span>
<span class="sd">                Formula: penalized_shap_mean = (mean_shap - (std_shap * shap_variance_penalty_factor))</span>

<span class="sd">            **shap_kwargs:</span>
<span class="sd">                keyword arguments passed to</span>
<span class="sd">                [shap.Explainer](https://shap.readthedocs.io/en/latest/generated/shap.Explainer.html#shap.Explainer).</span>
<span class="sd">                It also enables `approximate` and `check_additivity` parameters, passed while calculating SHAP values.</span>
<span class="sd">                The `approximate=True` causes less accurate, but faster SHAP values calculation, while</span>
<span class="sd">                `check_additivity=False` disables the additivity check inside SHAP.</span>

<span class="sd">        Returns:</span>
<span class="sd">            (pd.DataFrame):</span>
<span class="sd">                DataFrame containing results of feature elimination from each iteration.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
            <span class="n">X</span><span class="p">,</span>
            <span class="n">y</span><span class="p">,</span>
            <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
            <span class="n">columns_to_keep</span><span class="o">=</span><span class="n">columns_to_keep</span><span class="p">,</span>
            <span class="n">column_names</span><span class="o">=</span><span class="n">column_names</span><span class="p">,</span>
            <span class="n">shap_variance_penalty_factor</span><span class="o">=</span><span class="n">shap_variance_penalty_factor</span><span class="p">,</span>
            <span class="o">**</span><span class="n">shap_kwargs</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">X</span><span class="p">,</span>
        <span class="n">y</span><span class="p">,</span>
        <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">columns_to_keep</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">column_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">groups</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">shap_variance_penalty_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">shap_kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fits the object with the provided data.</span>

<span class="sd">        The algorithm starts with the entire dataset, and then sequentially</span>
<span class="sd">            eliminates features. If sklearn compatible search CV is passed as model e.g.</span>
<span class="sd">            [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html),</span>
<span class="sd">            [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)</span>
<span class="sd">            or [BayesSearchCV](https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html),</span>
<span class="sd">            the hyperparameter optimization is applied at each step of the elimination.</span>
<span class="sd">            Then, the SHAP feature importance is calculated using Cross-Validation,</span>
<span class="sd">            and `step` lowest importance features are removed.</span>

<span class="sd">        Args:</span>
<span class="sd">            X (pd.DataFrame):</span>
<span class="sd">                Provided dataset.</span>

<span class="sd">            y (pd.Series):</span>
<span class="sd">                Labels for X.</span>

<span class="sd">            sample_weight (pd.Series, np.ndarray, list, optional):</span>
<span class="sd">                array-like of shape (n_samples,) - only use if the model you&#39;re using supports</span>
<span class="sd">                sample weighting (check the corresponding scikit-learn documentation).</span>
<span class="sd">                Array of weights that are assigned to individual samples.</span>
<span class="sd">                Note that they&#39;re only used for fitting of  the model, not during evaluation of metrics.</span>
<span class="sd">                If not provided, then each sample is given unit weight.</span>

<span class="sd">            columns_to_keep (list of str, optional):</span>
<span class="sd">                List of column names to keep. If given,</span>
<span class="sd">                these columns will not be eliminated by the feature elimination process.</span>
<span class="sd">                However, these feature will used for the calculation of the SHAP values.</span>

<span class="sd">            column_names (list of str, optional):</span>
<span class="sd">                List of feature names of the provided samples. If provided it will be used to overwrite the existing</span>
<span class="sd">                feature names. If not provided the existing feature names are used or default feature names are</span>
<span class="sd">                generated.</span>

<span class="sd">            groups (pd.Series, np.ndarray, list, optional):</span>
<span class="sd">                array-like of shape (n_samples,)</span>
<span class="sd">                Group labels for the samples used while splitting the dataset into train/test set.</span>
<span class="sd">                Only used in conjunction with a &quot;Group&quot; `cv` instance.</span>
<span class="sd">                (e.g. `sklearn.model_selection.GroupKFold`).</span>

<span class="sd">            shap_variance_penalty_factor (int or float, optional):</span>
<span class="sd">                Apply aggregation penalty when computing average of shap values for a given feature.</span>
<span class="sd">                Results in a preference for features that have smaller standard deviation of shap</span>
<span class="sd">                values (more coherent shap importance). Recommend value 0.5 - 1.0.</span>
<span class="sd">                Formula: penalized_shap_mean = (mean_shap - (std_shap * shap_variance_penalty_factor))</span>

<span class="sd">            **shap_kwargs:</span>
<span class="sd">                keyword arguments passed to</span>
<span class="sd">                [shap.Explainer](https://shap.readthedocs.io/en/latest/generated/shap.Explainer.html#shap.Explainer).</span>
<span class="sd">                It also enables `approximate` and `check_additivity` parameters, passed while calculating SHAP values.</span>
<span class="sd">                The `approximate=True` causes less accurate, but faster SHAP values calculation, while</span>
<span class="sd">                `check_additivity=False` disables the additivity check inside SHAP.</span>

<span class="sd">        Returns:</span>
<span class="sd">            (ShapRFECV): Fitted object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Initialise len_columns_to_keep based on columns_to_keep content validation</span>
        <span class="n">len_columns_to_keep</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="n">columns_to_keep</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">columns_to_keep</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;All elements in columns_to_keep must be strings.&quot;</span><span class="p">)</span>
            <span class="n">len_columns_to_keep</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">columns_to_keep</span><span class="p">)</span>

        <span class="c1"># Validate matching column names, if both columns_to_keep and column_names are provided</span>
        <span class="k">if</span> <span class="n">column_names</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="n">x</span> <span class="ow">in</span> <span class="n">column_names</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Column names in columns_to_keep and column_names do not match.&quot;</span><span class="p">)</span>

        <span class="c1"># Validate total number of columns to select against the total number of columns</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">column_names</span>
            <span class="ow">and</span> <span class="n">columns_to_keep</span>
            <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_features_to_select</span> <span class="o">+</span> <span class="n">len_columns_to_keep</span><span class="p">)</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">column_names</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Minimum features to select plus columns_to_keep exceeds total number of features.&quot;</span><span class="p">)</span>

        <span class="c1"># Check shap_variance_penalty_factor has acceptable value</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shap_variance_penalty_factor</span><span class="p">,</span> <span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">))</span> <span class="ow">and</span> <span class="n">shap_variance_penalty_factor</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">_shap_variance_penalty_factor</span> <span class="o">=</span> <span class="n">shap_variance_penalty_factor</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">shap_variance_penalty_factor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="s2">&quot;shap_variance_penalty_factor must be None, int or float. Setting shap_variance_penalty_factor = 0&quot;</span>
                <span class="p">)</span>
            <span class="n">_shap_variance_penalty_factor</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">column_names</span> <span class="o">=</span> <span class="n">preprocess_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X_name</span><span class="o">=</span><span class="s2">&quot;X&quot;</span><span class="p">,</span> <span class="n">column_names</span><span class="o">=</span><span class="n">column_names</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">preprocess_labels</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_name</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="s2">&quot;sample_weight is passed only to the fit method of the model, not the evaluation metrics.&quot;</span>
                <span class="p">)</span>
            <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">assure_pandas_series</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cv</span> <span class="o">=</span> <span class="n">check_cv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cv</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">,</span> <span class="n">classifier</span><span class="o">=</span><span class="n">is_classifier</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">))</span>

        <span class="n">remaining_features</span> <span class="o">=</span> <span class="n">current_features_set</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">column_names</span>
        <span class="n">round_number</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># Stop when stopping criteria is met.</span>
        <span class="n">stopping_criteria</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">min_features_to_select</span><span class="p">,</span> <span class="n">len_columns_to_keep</span><span class="p">])</span>

        <span class="c1"># Setting up the min_features_to_select parameter.</span>
        <span class="k">if</span> <span class="n">columns_to_keep</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">min_features_to_select</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="c1"># Ensures that, if columns_to_keep is provided, the last features remaining are only the columns_to_keep.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Minimum features to select : </span><span class="si">{</span><span class="n">stopping_criteria</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">current_features_set</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">stopping_criteria</span><span class="p">:</span>
            <span class="n">round_number</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="c1"># Get current dataset info</span>
            <span class="n">current_features_set</span> <span class="o">=</span> <span class="n">remaining_features</span>
            <span class="n">remaining_removeable_features</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">dict</span><span class="o">.</span><span class="n">fromkeys</span><span class="p">(</span><span class="n">current_features_set</span> <span class="o">+</span> <span class="p">(</span><span class="n">columns_to_keep</span> <span class="ow">or</span> <span class="p">[])))</span>

            <span class="c1"># Current dataset</span>
            <span class="n">current_X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">[</span><span class="n">remaining_removeable_features</span><span class="p">]</span>

            <span class="c1"># Optimize parameters</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">search_model</span><span class="p">:</span>
                <span class="n">current_search_model</span> <span class="o">=</span> <span class="n">clone</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">current_X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">)</span>
                <span class="n">current_model</span> <span class="o">=</span> <span class="n">current_search_model</span><span class="o">.</span><span class="n">estimator</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="n">current_search_model</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">current_model</span> <span class="o">=</span> <span class="n">clone</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>

            <span class="c1"># Early stopping enabled (or not)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">early_stopping_rounds</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_metric</span><span class="p">):</span>
                <span class="c1"># Perform CV to estimate feature importance with SHAP</span>
                <span class="n">results_per_fold</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">)(</span>
                    <span class="n">delayed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_get_feature_shap_values_per_fold</span><span class="p">)(</span>
                        <span class="n">X</span><span class="o">=</span><span class="n">current_X</span><span class="p">,</span>
                        <span class="n">y</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">,</span>
                        <span class="n">model</span><span class="o">=</span><span class="n">current_model</span><span class="p">,</span>
                        <span class="n">train_index</span><span class="o">=</span><span class="n">train_index</span><span class="p">,</span>
                        <span class="n">val_index</span><span class="o">=</span><span class="n">val_index</span><span class="p">,</span>
                        <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
                        <span class="o">**</span><span class="n">shap_kwargs</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="k">for</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">val_index</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">cv</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">current_X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">,</span> <span class="n">groups</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Perform CV to estimate feature importance with SHAP</span>
                <span class="n">results_per_fold</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">)(</span>
                    <span class="n">delayed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_get_feature_shap_values_per_fold_early_stopping</span><span class="p">)(</span>
                        <span class="n">X</span><span class="o">=</span><span class="n">current_X</span><span class="p">,</span>
                        <span class="n">y</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">,</span>
                        <span class="n">model</span><span class="o">=</span><span class="n">current_model</span><span class="p">,</span>
                        <span class="n">train_index</span><span class="o">=</span><span class="n">train_index</span><span class="p">,</span>
                        <span class="n">val_index</span><span class="o">=</span><span class="n">val_index</span><span class="p">,</span>
                        <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
                        <span class="o">**</span><span class="n">shap_kwargs</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="k">for</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">val_index</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">cv</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">current_X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">,</span> <span class="n">groups</span><span class="p">)</span>
                <span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="o">.</span><span class="n">nunique</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">or</span> <span class="n">is_regressor</span><span class="p">(</span><span class="n">current_model</span><span class="p">):</span>
                <span class="n">shap_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">current_result</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">current_result</span> <span class="ow">in</span> <span class="n">results_per_fold</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>  <span class="c1"># multi-class case</span>
                <span class="n">shap_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">current_result</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">current_result</span> <span class="ow">in</span> <span class="n">results_per_fold</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

            <span class="n">scores_train</span> <span class="o">=</span> <span class="p">[</span><span class="n">current_result</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">current_result</span> <span class="ow">in</span> <span class="n">results_per_fold</span><span class="p">]</span>
            <span class="n">scores_val</span> <span class="o">=</span> <span class="p">[</span><span class="n">current_result</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">current_result</span> <span class="ow">in</span> <span class="n">results_per_fold</span><span class="p">]</span>

            <span class="c1"># Calculate the shap features with remaining features and features to keep.</span>
            <span class="n">shap_importance_df</span> <span class="o">=</span> <span class="n">calculate_shap_importance</span><span class="p">(</span>
                <span class="n">shap_values</span><span class="p">,</span> <span class="n">remaining_removeable_features</span><span class="p">,</span> <span class="n">shap_variance_penalty_factor</span><span class="o">=</span><span class="n">_shap_variance_penalty_factor</span>
            <span class="p">)</span>

            <span class="c1"># Determine which features to keep and which to remove.</span>
            <span class="n">remaining_features</span><span class="p">,</span> <span class="n">features_to_remove</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_filter_and_identify_features_based_on_importance</span><span class="p">(</span>
                <span class="n">shap_importance_df</span><span class="p">,</span> <span class="n">columns_to_keep</span><span class="p">,</span> <span class="n">current_features_set</span>
            <span class="p">)</span>

            <span class="c1"># Report results</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_report_current_results</span><span class="p">(</span>
                <span class="n">round_number</span><span class="o">=</span><span class="n">round_number</span><span class="p">,</span>
                <span class="n">current_features_set</span><span class="o">=</span><span class="n">current_features_set</span><span class="p">,</span>
                <span class="n">features_to_remove</span><span class="o">=</span><span class="n">features_to_remove</span><span class="p">,</span>
                <span class="n">train_metric_mean</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores_train</span><span class="p">),</span>
                <span class="n">train_metric_std</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">scores_train</span><span class="p">),</span>
                <span class="n">val_metric_mean</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores_val</span><span class="p">),</span>
                <span class="n">val_metric_std</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">scores_val</span><span class="p">),</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Round: </span><span class="si">{</span><span class="n">round_number</span><span class="si">}</span><span class="s2">, Current number of features: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">current_features_set</span><span class="p">)</span><span class="si">}</span><span class="s2">, &quot;</span>
                    <span class="sa">f</span><span class="s1">&#39;Current performance: Train </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">round_number</span><span class="p">][</span><span class="s2">&quot;train_metric_mean&quot;</span><span class="p">]</span><span class="si">}</span><span class="s1"> &#39;</span>
                    <span class="sa">f</span><span class="s1">&#39;+/- </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">round_number</span><span class="p">][</span><span class="s2">&quot;train_metric_std&quot;</span><span class="p">]</span><span class="si">}</span><span class="s1">, CV Validation &#39;</span>
                    <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">round_number</span><span class="p">][</span><span class="s2">&quot;val_metric_mean&quot;</span><span class="p">]</span><span class="si">}</span><span class="s1"> &#39;</span>
                    <span class="sa">f</span><span class="s1">&#39;+/- </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">round_number</span><span class="p">][</span><span class="s2">&quot;val_metric_std&quot;</span><span class="p">]</span><span class="si">}</span><span class="s1">. </span><span class="se">\n</span><span class="s1">&#39;</span>
                    <span class="sa">f</span><span class="s2">&quot;Features left: </span><span class="si">{</span><span class="n">remaining_features</span><span class="si">}</span><span class="s2">. &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;Removed features at the end of the round: </span><span class="si">{</span><span class="n">features_to_remove</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fitted</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">figure_kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generates plot of the model performance for each iteration of feature elimination.</span>

<span class="sd">        Args:</span>
<span class="sd">            show (bool, optional):</span>
<span class="sd">                If True, the plots are showed to the user, otherwise they are not shown. Not showing plot can be useful,</span>
<span class="sd">                when you want to edit the returned figure, before showing it.</span>

<span class="sd">            **figure_kwargs:</span>
<span class="sd">                Keyword arguments that are passed to the plt.figure, at its initialization.</span>

<span class="sd">        Returns:</span>
<span class="sd">            (plt.figure):</span>
<span class="sd">                Figure containing the performance plot.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Data preparation</span>
        <span class="n">num_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="p">[</span><span class="s2">&quot;num_features&quot;</span><span class="p">]</span>
        <span class="n">train_mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="p">[</span><span class="s2">&quot;train_metric_mean&quot;</span><span class="p">]</span>
        <span class="n">train_std</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="p">[</span><span class="s2">&quot;train_metric_std&quot;</span><span class="p">]</span>
        <span class="n">val_mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="p">[</span><span class="s2">&quot;val_metric_mean&quot;</span><span class="p">]</span>
        <span class="n">val_std</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="p">[</span><span class="s2">&quot;val_metric_std&quot;</span><span class="p">]</span>
        <span class="n">x_ticks</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">num_features</span><span class="o">.</span><span class="n">tolist</span><span class="p">()))</span>

        <span class="c1"># Plotting</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="o">**</span><span class="n">figure_kwargs</span><span class="p">)</span>

        <span class="c1"># Training performance</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">num_features</span><span class="p">,</span> <span class="n">train_mean</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Train Score&quot;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">num_features</span><span class="p">,</span> <span class="n">train_mean</span> <span class="o">-</span> <span class="n">train_std</span><span class="p">,</span> <span class="n">train_mean</span> <span class="o">+</span> <span class="n">train_std</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

        <span class="c1"># Validation performance</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">num_features</span><span class="p">,</span> <span class="n">val_mean</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Validation Score&quot;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">num_features</span><span class="p">,</span> <span class="n">val_mean</span> <span class="o">-</span> <span class="n">val_std</span><span class="p">,</span> <span class="n">val_mean</span> <span class="o">+</span> <span class="n">val_std</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

        <span class="c1"># Labels and title</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Number of features&quot;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Performance </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">scorer</span><span class="o">.</span><span class="n">metric_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Backwards Feature Elimination using SHAP &amp; CV&quot;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;lower left&quot;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">invert_xaxis</span><span class="p">()</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">x_ticks</span><span class="p">)</span>

        <span class="c1"># Display or close plot</span>
        <span class="k">if</span> <span class="n">show</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">(</span><span class="n">fig</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">fig</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_validate_step</span><span class="p">(</span><span class="n">step</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">))</span> <span class="ow">or</span> <span class="n">step</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Invalid step value: </span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s2">. Must be a positive int or float.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">step</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_validate_min_features</span><span class="p">(</span><span class="n">min_features</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">min_features</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="n">min_features</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Invalid min_features_to_select value: </span><span class="si">{</span><span class="n">min_features</span><span class="si">}</span><span class="s2">. Must be a positive int.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">min_features</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_calculate_number_of_features_to_remove</span><span class="p">(</span>
        <span class="n">current_num_of_features</span><span class="p">,</span>
        <span class="n">num_features_to_remove</span><span class="p">,</span>
        <span class="n">min_num_features_to_keep</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Calculates the number of features to be removed.</span>

<span class="sd">        Makes sure that after removal at least</span>
<span class="sd">            min_num_features_to_keep are kept</span>

<span class="sd">        Args:</span>
<span class="sd">            current_num_of_features (int):</span>
<span class="sd">                Current number of features in the data.</span>

<span class="sd">            num_features_to_remove (int):</span>
<span class="sd">                Number of features to be removed at this stage.</span>

<span class="sd">            min_num_features_to_keep (int):</span>
<span class="sd">                Minimum number of features to be left after removal.</span>

<span class="sd">        Returns:</span>
<span class="sd">            (int):</span>
<span class="sd">                Number of features to be removed.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Calculate maximum nr of features that can be removed without dropping below</span>
        <span class="c1"># `min_num_features_to_keep`.</span>
        <span class="n">nr_of_max_allowed_feature_removed</span> <span class="o">=</span> <span class="n">current_num_of_features</span> <span class="o">-</span> <span class="n">min_num_features_to_keep</span>

        <span class="c1"># Return smallest between `nr_of_max_allowed_feature_removed` and `num_features_to_remove`</span>
        <span class="k">return</span> <span class="nb">min</span><span class="p">(</span><span class="n">num_features_to_remove</span><span class="p">,</span> <span class="n">nr_of_max_allowed_feature_removed</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_current_features_to_remove</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shap_importance_df</span><span class="p">,</span> <span class="n">columns_to_keep</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Implements the logic used to determine which features to remove.</span>

<span class="sd">        If step is a positive integer,</span>
<span class="sd">            at each round step lowest SHAP importance features are selected. If it is a float, such percentage</span>
<span class="sd">            of remaining features (rounded up) is removed each iteration. It is recommended to use float, since it is</span>
<span class="sd">            faster for a large set of features, and slows down and becomes more precise with fewer features.</span>

<span class="sd">        Args:</span>
<span class="sd">            shap_importance_df (pd.DataFrame):</span>
<span class="sd">                DataFrame presenting SHAP importance of remaining features.</span>

<span class="sd">            columns_to_keep Optional(list)L</span>
<span class="sd">                A list of features that are kept.</span>

<span class="sd">        Returns:</span>
<span class="sd">            (list):</span>
<span class="sd">                List of features to be removed at a given round.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Bounding the variable.</span>
        <span class="n">num_features_to_remove</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># If columns_to_keep is not None, exclude those columns and</span>
        <span class="c1"># calculate features to remove.</span>
        <span class="k">if</span> <span class="n">columns_to_keep</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">shap_importance_df</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">columns_to_keep</span><span class="p">)</span>
            <span class="n">shap_importance_df</span> <span class="o">=</span> <span class="n">shap_importance_df</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">]</span>

        <span class="c1"># If the step is an int remove n features.</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">step</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">num_features_to_remove</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calculate_number_of_features_to_remove</span><span class="p">(</span>
                <span class="n">current_num_of_features</span><span class="o">=</span><span class="n">shap_importance_df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">num_features_to_remove</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">step</span><span class="p">,</span>
                <span class="n">min_num_features_to_keep</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">min_features_to_select</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="c1"># If the step is a float remove n * number features that are left, rounded down</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">step</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
            <span class="n">current_step</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">shap_importance_df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">step</span><span class="p">))</span>
            <span class="c1"># The step after rounding down should be at least 1</span>
            <span class="k">if</span> <span class="n">current_step</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">current_step</span> <span class="o">=</span> <span class="mi">1</span>

            <span class="n">num_features_to_remove</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calculate_number_of_features_to_remove</span><span class="p">(</span>
                <span class="n">current_num_of_features</span><span class="o">=</span><span class="n">shap_importance_df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">num_features_to_remove</span><span class="o">=</span><span class="n">current_step</span><span class="p">,</span>
                <span class="n">min_num_features_to_keep</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">min_features_to_select</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">num_features_to_remove</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">shap_importance_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="o">-</span><span class="n">num_features_to_remove</span><span class="p">:]</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_report_current_results</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">round_number</span><span class="p">,</span>
        <span class="n">current_features_set</span><span class="p">,</span>
        <span class="n">features_to_remove</span><span class="p">,</span>
        <span class="n">train_metric_mean</span><span class="p">,</span>
        <span class="n">train_metric_std</span><span class="p">,</span>
        <span class="n">val_metric_mean</span><span class="p">,</span>
        <span class="n">val_metric_std</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This function adds the results from a current iteration to the report.</span>

<span class="sd">        Args:</span>
<span class="sd">            round_number (int):</span>
<span class="sd">                Current number of the round.</span>

<span class="sd">            current_features_set (list of str):</span>
<span class="sd">                Current list of features.</span>

<span class="sd">            features_to_remove (list of str):</span>
<span class="sd">                List of features to be removed at the end of this iteration.</span>

<span class="sd">            train_metric_mean (float or int):</span>
<span class="sd">                Mean scoring metric measured on train set during CV.</span>

<span class="sd">            train_metric_std (float or int):</span>
<span class="sd">                Std scoring metric measured on train set during CV.</span>

<span class="sd">            val_metric_mean (float or int):</span>
<span class="sd">                Mean scoring metric measured on validation set during CV.</span>

<span class="sd">            val_metric_std (float or int):</span>
<span class="sd">                Std scoring metric measured on validation set during CV.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">current_results</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;num_features&quot;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">current_features_set</span><span class="p">),</span>
            <span class="s2">&quot;features_set&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">current_features_set</span><span class="p">],</span>
            <span class="s2">&quot;eliminated_features&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">features_to_remove</span><span class="p">],</span>
            <span class="s2">&quot;train_metric_mean&quot;</span><span class="p">:</span> <span class="n">train_metric_mean</span><span class="p">,</span>
            <span class="s2">&quot;train_metric_std&quot;</span><span class="p">:</span> <span class="n">train_metric_std</span><span class="p">,</span>
            <span class="s2">&quot;val_metric_mean&quot;</span><span class="p">:</span> <span class="n">val_metric_mean</span><span class="p">,</span>
            <span class="s2">&quot;val_metric_std&quot;</span><span class="p">:</span> <span class="n">val_metric_std</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="o">.</span><span class="n">empty</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">current_results</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="n">round_number</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">new_row</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">current_results</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="n">round_number</span><span class="p">])</span>
            <span class="c1"># Append new_row to self.report_df more efficiently</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="p">,</span> <span class="n">new_row</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">_get_feature_shap_values_per_fold</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">X</span><span class="p">,</span>
        <span class="n">y</span><span class="p">,</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">train_index</span><span class="p">,</span>
        <span class="n">val_index</span><span class="p">,</span>
        <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">shap_kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This function calculates the shap values on validation set, and Train and Val score.</span>

<span class="sd">        Args:</span>
<span class="sd">            X (pd.DataFrame):</span>
<span class="sd">                Dataset used in CV.</span>

<span class="sd">            y (pd.Series):</span>
<span class="sd">                Labels for X.</span>

<span class="sd">            model (classifier or regressor):</span>
<span class="sd">                Model to be fitted on the train folds.</span>

<span class="sd">            train_index (np.array):</span>
<span class="sd">                Positions of train folds samples.</span>

<span class="sd">            val_index (np.array):</span>
<span class="sd">                Positions of validation fold samples.</span>

<span class="sd">            sample_weight (pd.Series, np.ndarray, list, optional):</span>
<span class="sd">                array-like of shape (n_samples,) - only use if the model you&#39;re using supports</span>
<span class="sd">                sample weighting (check the corresponding scikit-learn documentation).</span>
<span class="sd">                Array of weights that are assigned to individual samples.</span>
<span class="sd">                Note that they&#39;re only used for fitting of  the model, not during evaluation of metrics.</span>
<span class="sd">                If not provided, then each sample is given unit weight.</span>

<span class="sd">            **shap_kwargs:</span>
<span class="sd">                keyword arguments passed to</span>
<span class="sd">                [shap.Explainer](https://shap.readthedocs.io/en/latest/generated/shap.Explainer.html#shap.Explainer).</span>
<span class="sd">                It also enables `approximate` and `check_additivity` parameters, passed while calculating SHAP values.</span>
<span class="sd">                The `approximate=True` causes less accurate, but faster SHAP values calculation, while</span>
<span class="sd">                `check_additivity=False` disables the additivity check inside SHAP.</span>
<span class="sd">        Returns:</span>
<span class="sd">            (np.array, float, float):</span>
<span class="sd">                Tuple with the results: Shap Values on validation fold, train score, validation score.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">X_train</span><span class="p">,</span> <span class="n">X_val</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">train_index</span><span class="p">,</span> <span class="p">:],</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">val_index</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">y_train</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">val_index</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">train_index</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

        <span class="c1"># Score the model</span>
        <span class="n">score_train</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scorer</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
        <span class="n">score_val</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scorer</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">)</span>

        <span class="c1"># Compute SHAP values</span>
        <span class="n">shap_values</span> <span class="o">=</span> <span class="n">shap_calc</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span> <span class="o">**</span><span class="n">shap_kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">shap_values</span><span class="p">,</span> <span class="n">score_train</span><span class="p">,</span> <span class="n">score_val</span>

    <span class="k">def</span> <span class="nf">_filter_and_identify_features_based_on_importance</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">shap_importance_df</span><span class="p">,</span> <span class="n">columns_to_keep</span><span class="p">,</span> <span class="n">current_features_set</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Filters out features to be removed from the current feature set based on SHAP importance,</span>
<span class="sd">        while maintaining the original order of the features.</span>

<span class="sd">        Args:</span>
<span class="sd">            shap_importance_df (pd.DataFrame):</span>
<span class="sd">                A DataFrame containing the SHAP importance of the features.</span>

<span class="sd">            columns_to_keep (list):</span>
<span class="sd">                A list of column names that should not be removed, regardless of their</span>
<span class="sd">                SHAP importance.</span>

<span class="sd">            current_features_set (list):</span>
<span class="sd">                The current list of features from which features identified as</span>
<span class="sd">                less important will be removed. This list&#39;s order is maintained in the</span>
<span class="sd">                returned list of remaining features.</span>

<span class="sd">        Returns:</span>
<span class="sd">            remaining_features, features_to_remove (list, list): The features to keep &amp; those that are removed.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Get features to remove based on SHAP importance and columns to keep</span>
        <span class="n">features_to_remove</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_current_features_to_remove</span><span class="p">(</span><span class="n">shap_importance_df</span><span class="p">,</span> <span class="n">columns_to_keep</span><span class="o">=</span><span class="n">columns_to_keep</span><span class="p">)</span>

        <span class="c1"># Convert features_to_remove to a set for O(1) lookup times</span>
        <span class="n">features_to_remove_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">features_to_remove</span><span class="p">)</span>

        <span class="c1"># Filter out the features to remove, maintaining the original order of current_features_set</span>
        <span class="n">remaining_features</span> <span class="o">=</span> <span class="p">[</span><span class="n">feature</span> <span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">current_features_set</span> <span class="k">if</span> <span class="n">feature</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">features_to_remove_set</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">remaining_features</span><span class="p">,</span> <span class="n">features_to_remove</span>

    <span class="k">def</span> <span class="nf">get_reduced_features_set</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_features</span><span class="p">,</span> <span class="n">standard_error_threshold</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s2">&quot;feature_names&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gets the features set after the feature elimination process, for a given number of features.</span>

<span class="sd">        Args:</span>
<span class="sd">            num_features (int or str):</span>
<span class="sd">                If int: Number of features in the reduced features set.</span>
<span class="sd">                If str: One of the following automatic num feature selection methods supported:</span>
<span class="sd">                    1. best: strictly selects the num_features with the highest model score.</span>
<span class="sd">                    2. best_coherent: For iterations that are within standard_error_threshold of the highest</span>
<span class="sd">                    score, select the iteration with the lowest standard deviation of model score.</span>
<span class="sd">                    3. best_parsimonious: For iterations that are within standard_error_threshold of the</span>
<span class="sd">                    highest score, select the iteration with the fewest features.</span>

<span class="sd">            standard_error_threshold (float):</span>
<span class="sd">                If num_features is &#39;best_coherent&#39; or &#39;best_parsimonious&#39;, this parameter is used.</span>

<span class="sd">            return_type:</span>
<span class="sd">                Accepts possible values of &#39;feature_names&#39;, &#39;support&#39; or &#39;ranking&#39;. These are defined as:</span>
<span class="sd">                    1. feature_names: returns column names</span>
<span class="sd">                    2. support: returns boolean mask</span>
<span class="sd">                    3. ranking: returns numeric ranking of features</span>

<span class="sd">        Returns:</span>
<span class="sd">            (list of str):</span>
<span class="sd">                Reduced features set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_if_fitted</span><span class="p">()</span>

        <span class="c1"># Determine the best number of features based on the method specified</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">num_features</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">num_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_best_num_features</span><span class="p">(</span>
                <span class="n">best_method</span><span class="o">=</span><span class="n">num_features</span><span class="p">,</span> <span class="n">standard_error_threshold</span><span class="o">=</span><span class="n">standard_error_threshold</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">num_features</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Parameter num_features can be of type int, or of type str with &quot;</span>
                <span class="s2">&quot;possible values of &#39;best&#39;, &#39;best_coherent&#39; or &#39;best_parsimonious&#39;&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Get feature names for the determined number of features</span>
        <span class="n">feature_names_selected</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_feature_names</span><span class="p">(</span><span class="n">num_features</span><span class="p">)</span>

        <span class="c1"># Return based on the requested return type</span>
        <span class="k">if</span> <span class="n">return_type</span> <span class="o">==</span> <span class="s2">&quot;feature_names&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">feature_names_selected</span>
        <span class="k">elif</span> <span class="n">return_type</span> <span class="o">==</span> <span class="s2">&quot;support&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_feature_support</span><span class="p">(</span><span class="n">feature_names_selected</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">return_type</span> <span class="o">==</span> <span class="s2">&quot;ranking&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_feature_ranking</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid return_type. Must be &#39;feature_names&#39;, &#39;support&#39;, or &#39;ranking&#39;.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_best_num_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">best_method</span><span class="p">,</span> <span class="n">standard_error_threshold</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Helper function to identify the best number of features to select as per some automatic</span>
<span class="sd">        feature selection strategy. Strategies supported are:</span>
<span class="sd">            1. best: strictly selects the num_features with the highest model score.</span>
<span class="sd">            2. best_coherent: For iterations that are within standard_error_threshold of the highest</span>
<span class="sd">            score, select the iteration with the lowest standard deviation of model score.</span>
<span class="sd">            3. best_parsimonious: For iterations that are within standard_error_threshold of the</span>
<span class="sd">            highest score, select the iteration with the fewest features.</span>

<span class="sd">        Args:</span>
<span class="sd">            best_method (str):</span>
<span class="sd">                Automatic best feature selection strategy. One of &quot;best&quot;, &quot;best_coherent&quot; or</span>
<span class="sd">                &quot;best_parsimonious&quot;.</span>

<span class="sd">            standard_error_threshold (float):</span>
<span class="sd">                Parameter used if best_method is &#39;best_coherent&#39; or &#39;best_parsimonious&#39;.</span>
<span class="sd">                Numeric value greater than zero.</span>

<span class="sd">        Returns:</span>
<span class="sd">            (int)</span>
<span class="sd">                num_features as per automatic feature selection strategy selected.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_if_fitted</span><span class="p">()</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">standard_error_threshold</span><span class="p">,</span> <span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">))</span> <span class="ow">or</span> <span class="n">standard_error_threshold</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Parameter standard_error_threshold must be a non-negative int or float.&quot;</span><span class="p">)</span>

        <span class="c1"># Perform copy after ValueError check.</span>
        <span class="n">shap_report</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">best_method</span> <span class="o">==</span> <span class="s2">&quot;best&quot;</span><span class="p">:</span>
            <span class="c1"># Strictly selects the number of features with the highest model score</span>
            <span class="n">best_score_index</span> <span class="o">=</span> <span class="n">shap_report</span><span class="p">[</span><span class="s2">&quot;val_metric_mean&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">idxmax</span><span class="p">()</span>
            <span class="n">best_num_features</span> <span class="o">=</span> <span class="n">shap_report</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">best_score_index</span><span class="p">,</span> <span class="s2">&quot;num_features&quot;</span><span class="p">]</span>

        <span class="k">elif</span> <span class="n">best_method</span> <span class="o">==</span> <span class="s2">&quot;best_coherent&quot;</span><span class="p">:</span>
            <span class="c1"># Selects within a threshold but prioritizes lower standard deviation</span>
            <span class="n">highest_score</span> <span class="o">=</span> <span class="n">shap_report</span><span class="p">[</span><span class="s2">&quot;val_metric_mean&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
            <span class="n">within_threshold</span> <span class="o">=</span> <span class="n">shap_report</span><span class="p">[</span><span class="n">shap_report</span><span class="p">[</span><span class="s2">&quot;val_metric_mean&quot;</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">highest_score</span> <span class="o">-</span> <span class="n">standard_error_threshold</span><span class="p">]</span>
            <span class="n">lowest_std_index</span> <span class="o">=</span> <span class="n">within_threshold</span><span class="p">[</span><span class="s2">&quot;val_metric_std&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">idxmin</span><span class="p">()</span>
            <span class="n">best_num_features</span> <span class="o">=</span> <span class="n">within_threshold</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">lowest_std_index</span><span class="p">,</span> <span class="s2">&quot;num_features&quot;</span><span class="p">]</span>

        <span class="k">elif</span> <span class="n">best_method</span> <span class="o">==</span> <span class="s2">&quot;best_parsimonious&quot;</span><span class="p">:</span>
            <span class="c1"># Selects the fewest number of features within the threshold of the highest score</span>
            <span class="n">highest_score</span> <span class="o">=</span> <span class="n">shap_report</span><span class="p">[</span><span class="s2">&quot;val_metric_mean&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
            <span class="n">within_threshold</span> <span class="o">=</span> <span class="n">shap_report</span><span class="p">[</span><span class="n">shap_report</span><span class="p">[</span><span class="s2">&quot;val_metric_mean&quot;</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">highest_score</span> <span class="o">-</span> <span class="n">standard_error_threshold</span><span class="p">]</span>
            <span class="n">fewest_features_index</span> <span class="o">=</span> <span class="n">within_threshold</span><span class="p">[</span><span class="s2">&quot;num_features&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">idxmin</span><span class="p">()</span>
            <span class="n">best_num_features</span> <span class="o">=</span> <span class="n">within_threshold</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">fewest_features_index</span><span class="p">,</span> <span class="s2">&quot;num_features&quot;</span><span class="p">]</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;The parameter &#39;best_method&#39; must be one of &#39;best&#39;, &#39;best_coherent&#39;, or &#39;best_parsimonious&#39;.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Log shap_report for users who want to inspect / debug</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="n">shap_report</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">best_num_features</span>

    <span class="k">def</span> <span class="nf">_get_feature_names</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_features</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Helper function that takes num_features and returns the associated list of column/feature names.</span>

<span class="sd">        Args:</span>
<span class="sd">            num_features (int):</span>
<span class="sd">                Represents the top N features to get the column names for.</span>

<span class="sd">        Returns:</span>
<span class="sd">            (list of feature names)</span>
<span class="sd">                List of the names of the features representing top num_features</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_if_fitted</span><span class="p">()</span>

        <span class="c1"># Direct lookup for the row with the desired number of features</span>
        <span class="n">matching_rows</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="o">.</span><span class="n">num_features</span> <span class="o">==</span> <span class="n">num_features</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">matching_rows</span><span class="o">.</span><span class="n">empty</span><span class="p">:</span>
            <span class="n">valid_nums</span> <span class="o">=</span> <span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="o">.</span><span class="n">num_features</span><span class="o">.</span><span class="n">unique</span><span class="p">())])</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The provided number of features has not been achieved at any stage of the process. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;You can select one of the following: </span><span class="si">{</span><span class="n">valid_nums</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Assuming &#39;features_set&#39; contains the list of feature names for the row</span>
        <span class="k">return</span> <span class="n">matching_rows</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;features_set&quot;</span><span class="p">]</span>

        <span class="c1"># Assuming &#39;features_set&#39; contains the list of feature names for the row</span>
        <span class="k">return</span> <span class="n">matching_rows</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;features_set&quot;</span><span class="p">]</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_get_feature_support</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feature_names_selected</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Helper function that takes feature_names_selected and returns a boolean mask representing the columns</span>
<span class="sd">        that were selected by the RFECV method.</span>

<span class="sd">        Args:</span>
<span class="sd">            feature_names_selected (list):</span>
<span class="sd">                Represents the top N features to get the column names for.</span>

<span class="sd">        Returns:</span>
<span class="sd">            (list of bools)</span>
<span class="sd">                Boolean mask representing the features selected.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">support</span> <span class="o">=</span> <span class="p">[</span><span class="kc">True</span> <span class="k">if</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">feature_names_selected</span> <span class="k">else</span> <span class="kc">False</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">column_names</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">support</span>

    <span class="k">def</span> <span class="nf">_get_feature_ranking</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the feature ranking, such that ranking_[i] corresponds to the ranking position</span>
<span class="sd">        of the i-th feature. Selected (i.e., estimated best) features are assigned rank 1.</span>

<span class="sd">        Returns:</span>
<span class="sd">            (list of bools)</span>
<span class="sd">                Boolean mask representing the features selected.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">flipped_report_df</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># Some features are not eliminated. All have importance of zero (highest importance)</span>
        <span class="n">features_not_eliminated</span> <span class="o">=</span> <span class="n">flipped_report_df</span><span class="p">[</span><span class="s2">&quot;features_set&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">features_not_eliminated_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">v</span><span class="p">:</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">features_not_eliminated</span><span class="p">}</span>

        <span class="c1"># Eliminated features are ranked by shap importance</span>
        <span class="n">features_eliminated</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">flipped_report_df</span><span class="p">[</span><span class="s2">&quot;eliminated_features&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">())</span>
        <span class="n">features_eliminated_dict</span> <span class="o">=</span> <span class="p">{</span><span class="nb">int</span><span class="p">(</span><span class="n">v</span><span class="p">):</span> <span class="n">k</span> <span class="o">+</span> <span class="mi">1</span> <span class="k">for</span> <span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">features_eliminated</span><span class="p">)}</span>

        <span class="c1"># Combine dicts with rank info</span>
        <span class="n">features_eliminated_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">features_not_eliminated_dict</span><span class="p">)</span>

        <span class="c1"># Get ranking per the order of columns</span>
        <span class="n">ranking</span> <span class="o">=</span> <span class="p">[</span><span class="n">features_eliminated_dict</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">column_names</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">ranking</span>

    <span class="k">def</span> <span class="nf">_get_fit_params_lightGBM</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">train_index</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">val_index</span><span class="o">=</span><span class="kc">None</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the fit parameters for for a LightGBM Model.</span>

<span class="sd">        Args:</span>

<span class="sd">            X_train (pd.DataFrame):</span>
<span class="sd">                Train Dataset used in CV.</span>

<span class="sd">            y_train (pd.Series):</span>
<span class="sd">                Train labels for X.</span>

<span class="sd">            X_val (pd.DataFrame):</span>
<span class="sd">                Validation Dataset used in CV.</span>

<span class="sd">            y_val (pd.Series):</span>
<span class="sd">                Validation labels for X.</span>

<span class="sd">            sample_weight (pd.Series, np.ndarray, list, optional):</span>
<span class="sd">                array-like of shape (n_samples,) - only use if the model you&#39;re using supports</span>
<span class="sd">                sample weighting (check the corresponding scikit-learn documentation).</span>
<span class="sd">                Array of weights that are assigned to individual samples.</span>
<span class="sd">                Note that they&#39;re only used for fitting of  the model, not during evaluation of metrics.</span>
<span class="sd">                If not provided, then each sample is given unit weight.</span>

<span class="sd">            train_index (np.array):</span>
<span class="sd">                Positions of train folds samples.</span>

<span class="sd">            val_index (np.array):</span>
<span class="sd">                Positions of validation fold samples.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: if the model is not supported.</span>

<span class="sd">        Returns:</span>
<span class="sd">            dict: fit parameters</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">lightgbm</span> <span class="kn">import</span> <span class="n">early_stopping</span><span class="p">,</span> <span class="n">log_evaluation</span>

        <span class="n">fit_params</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;X&quot;</span><span class="p">:</span> <span class="n">X_train</span><span class="p">,</span>
            <span class="s2">&quot;y&quot;</span><span class="p">:</span> <span class="n">y_train</span><span class="p">,</span>
            <span class="s2">&quot;eval_set&quot;</span><span class="p">:</span> <span class="p">[(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">)],</span>
            <span class="s2">&quot;eval_metric&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_metric</span><span class="p">,</span>
            <span class="s2">&quot;callbacks&quot;</span><span class="p">:</span> <span class="p">[</span>
                <span class="n">early_stopping</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">early_stopping_rounds</span><span class="p">,</span> <span class="n">first_metric_only</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                <span class="n">log_evaluation</span><span class="p">(</span><span class="mi">1</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">&gt;=</span> <span class="mi">2</span> <span class="k">else</span> <span class="mi">0</span><span class="p">),</span>
            <span class="p">],</span>
        <span class="p">}</span>

        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">fit_params</span><span class="p">[</span><span class="s2">&quot;sample_weight&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sample_weight</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">train_index</span><span class="p">]</span>
            <span class="n">fit_params</span><span class="p">[</span><span class="s2">&quot;eval_sample_weight&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">sample_weight</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">val_index</span><span class="p">]]</span>

        <span class="k">return</span> <span class="n">fit_params</span>

    <span class="k">def</span> <span class="nf">_get_fit_params_XGBoost</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">train_index</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">val_index</span><span class="o">=</span><span class="kc">None</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the fit parameters for for a XGBoost Model.</span>

<span class="sd">        Args:</span>

<span class="sd">            X_train (pd.DataFrame):</span>
<span class="sd">                Train Dataset used in CV.</span>

<span class="sd">            y_train (pd.Series):</span>
<span class="sd">                Train labels for X.</span>

<span class="sd">            X_val (pd.DataFrame):</span>
<span class="sd">                Validation Dataset used in CV.</span>

<span class="sd">            y_val (pd.Series):</span>
<span class="sd">                Validation labels for X.</span>

<span class="sd">            sample_weight (pd.Series, np.ndarray, list, optional):</span>
<span class="sd">                array-like of shape (n_samples,) - only use if the model you&#39;re using supports</span>
<span class="sd">                sample weighting (check the corresponding scikit-learn documentation).</span>
<span class="sd">                Array of weights that are assigned to individual samples.</span>
<span class="sd">                Note that they&#39;re only used for fitting of  the model, not during evaluation of metrics.</span>
<span class="sd">                If not provided, then each sample is given unit weight.</span>

<span class="sd">            train_index (np.array):</span>
<span class="sd">                Positions of train folds samples.</span>

<span class="sd">            val_index (np.array):</span>
<span class="sd">                Positions of validation fold samples.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: if the model is not supported.</span>

<span class="sd">        Returns:</span>
<span class="sd">            dict: fit parameters</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">fit_params</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;X&quot;</span><span class="p">:</span> <span class="n">X_train</span><span class="p">,</span>
            <span class="s2">&quot;y&quot;</span><span class="p">:</span> <span class="n">y_train</span><span class="p">,</span>
            <span class="s2">&quot;eval_set&quot;</span><span class="p">:</span> <span class="p">[(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">)],</span>
        <span class="p">}</span>
        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">fit_params</span><span class="p">[</span><span class="s2">&quot;sample_weight&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sample_weight</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">train_index</span><span class="p">]</span>
            <span class="n">fit_params</span><span class="p">[</span><span class="s2">&quot;eval_sample_weight&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">sample_weight</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">val_index</span><span class="p">]]</span>

        <span class="k">return</span> <span class="n">fit_params</span>

    <span class="k">def</span> <span class="nf">_get_fit_params_CatBoost</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">train_index</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">val_index</span><span class="o">=</span><span class="kc">None</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the fit parameters for for a CatBoost Model.</span>

<span class="sd">        Args:</span>

<span class="sd">            X_train (pd.DataFrame):</span>
<span class="sd">                Train Dataset used in CV.</span>

<span class="sd">            y_train (pd.Series):</span>
<span class="sd">                Train labels for X.</span>

<span class="sd">            X_val (pd.DataFrame):</span>
<span class="sd">                Validation Dataset used in CV.</span>

<span class="sd">            y_val (pd.Series):</span>
<span class="sd">                Validation labels for X.</span>

<span class="sd">            sample_weight (pd.Series, np.ndarray, list, optional):</span>
<span class="sd">                array-like of shape (n_samples,) - only use if the model you&#39;re using supports</span>
<span class="sd">                sample weighting (check the corresponding scikit-learn documentation).</span>
<span class="sd">                Array of weights that are assigned to individual samples.</span>
<span class="sd">                Note that they&#39;re only used for fitting of  the model, not during evaluation of metrics.</span>
<span class="sd">                If not provided, then each sample is given unit weight.</span>

<span class="sd">            train_index (np.array):</span>
<span class="sd">                Positions of train folds samples.</span>

<span class="sd">            val_index (np.array):</span>
<span class="sd">                Positions of validation fold samples.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: if the model is not supported.</span>

<span class="sd">        Returns:</span>
<span class="sd">            dict: fit parameters</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">catboost</span> <span class="kn">import</span> <span class="n">Pool</span>

        <span class="n">cat_features</span> <span class="o">=</span> <span class="p">[</span><span class="n">col</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">X_train</span><span class="o">.</span><span class="n">select_dtypes</span><span class="p">(</span><span class="n">include</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;category&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">columns</span><span class="p">]</span>
        <span class="n">fit_params</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;X&quot;</span><span class="p">:</span> <span class="n">Pool</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cat_features</span><span class="o">=</span><span class="n">cat_features</span><span class="p">),</span>
            <span class="s2">&quot;eval_set&quot;</span><span class="p">:</span> <span class="n">Pool</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">cat_features</span><span class="o">=</span><span class="n">cat_features</span><span class="p">),</span>
            <span class="c1"># Evaluation metric should be passed during initialization</span>
        <span class="p">}</span>
        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">fit_params</span><span class="p">[</span><span class="s2">&quot;X&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">set_weight</span><span class="p">(</span><span class="n">sample_weight</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">train_index</span><span class="p">])</span>
            <span class="n">fit_params</span><span class="p">[</span><span class="s2">&quot;eval_set&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">set_weight</span><span class="p">(</span><span class="n">sample_weight</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">val_index</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">fit_params</span>

    <span class="k">def</span> <span class="nf">_get_fit_params</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">train_index</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">val_index</span><span class="o">=</span><span class="kc">None</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the fit parameters for the specified classifier or regressor.</span>

<span class="sd">        Args:</span>
<span class="sd">            model (classifier or regressor):</span>
<span class="sd">                Model to be fitted on the train folds.</span>

<span class="sd">            X_train (pd.DataFrame):</span>
<span class="sd">                Train Dataset used in CV.</span>

<span class="sd">            y_train (pd.Series):</span>
<span class="sd">                Train labels for X.</span>

<span class="sd">            X_val (pd.DataFrame):</span>
<span class="sd">                Validation Dataset used in CV.</span>

<span class="sd">            y_val (pd.Series):</span>
<span class="sd">                Validation labels for X.</span>

<span class="sd">            sample_weight (pd.Series, np.ndarray, list, optional):</span>
<span class="sd">                array-like of shape (n_samples,) - only use if the model you&#39;re using supports</span>
<span class="sd">                sample weighting (check the corresponding scikit-learn documentation).</span>
<span class="sd">                Array of weights that are assigned to individual samples.</span>
<span class="sd">                Note that they&#39;re only used for fitting of  the model, not during evaluation of metrics.</span>
<span class="sd">                If not provided, then each sample is given unit weight.</span>

<span class="sd">            train_index (np.array):</span>
<span class="sd">                Positions of train folds samples.</span>

<span class="sd">            val_index (np.array):</span>
<span class="sd">                Positions of validation fold samples.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: if the model is not supported.</span>

<span class="sd">        Returns:</span>
<span class="sd">            dict: fit parameters</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="kn">from</span> <span class="nn">lightgbm</span> <span class="kn">import</span> <span class="n">LGBMModel</span>

            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">LGBMModel</span><span class="p">):</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_fit_params_lightGBM</span><span class="p">(</span>
                    <span class="n">X_train</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span>
                    <span class="n">y_train</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span>
                    <span class="n">X_val</span><span class="o">=</span><span class="n">X_val</span><span class="p">,</span>
                    <span class="n">y_val</span><span class="o">=</span><span class="n">y_val</span><span class="p">,</span>
                    <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
                    <span class="n">train_index</span><span class="o">=</span><span class="n">train_index</span><span class="p">,</span>
                    <span class="n">val_index</span><span class="o">=</span><span class="n">val_index</span><span class="p">,</span>
                <span class="p">)</span>
        <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
            <span class="k">pass</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="kn">from</span> <span class="nn">xgboost.sklearn</span> <span class="kn">import</span> <span class="n">XGBModel</span>

            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">XGBModel</span><span class="p">):</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_fit_params_XGBoost</span><span class="p">(</span>
                    <span class="n">X_train</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span>
                    <span class="n">y_train</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span>
                    <span class="n">X_val</span><span class="o">=</span><span class="n">X_val</span><span class="p">,</span>
                    <span class="n">y_val</span><span class="o">=</span><span class="n">y_val</span><span class="p">,</span>
                    <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
                    <span class="n">train_index</span><span class="o">=</span><span class="n">train_index</span><span class="p">,</span>
                    <span class="n">val_index</span><span class="o">=</span><span class="n">val_index</span><span class="p">,</span>
                <span class="p">)</span>
        <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
            <span class="k">pass</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="kn">from</span> <span class="nn">catboost</span> <span class="kn">import</span> <span class="n">CatBoost</span>

            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">CatBoost</span><span class="p">):</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_fit_params_CatBoost</span><span class="p">(</span>
                    <span class="n">X_train</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span>
                    <span class="n">y_train</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span>
                    <span class="n">X_val</span><span class="o">=</span><span class="n">X_val</span><span class="p">,</span>
                    <span class="n">y_val</span><span class="o">=</span><span class="n">y_val</span><span class="p">,</span>
                    <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
                    <span class="n">train_index</span><span class="o">=</span><span class="n">train_index</span><span class="p">,</span>
                    <span class="n">val_index</span><span class="o">=</span><span class="n">val_index</span><span class="p">,</span>
                <span class="p">)</span>
        <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
            <span class="k">pass</span>

        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Model type not supported&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_feature_shap_values_per_fold_early_stopping</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">X</span><span class="p">,</span>
        <span class="n">y</span><span class="p">,</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">train_index</span><span class="p">,</span>
        <span class="n">val_index</span><span class="p">,</span>
        <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">shap_kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This function calculates the shap values on validation set, and Train and Val score.</span>

<span class="sd">        Args:</span>
<span class="sd">            X (pd.DataFrame):</span>
<span class="sd">                Dataset used in CV.</span>

<span class="sd">            y (pd.Series):</span>
<span class="sd">                Labels for X.</span>

<span class="sd">            sample_weight (pd.Series, np.ndarray, list, optional):</span>
<span class="sd">                array-like of shape (n_samples,) - only use if the model you&#39;re using supports</span>
<span class="sd">                sample weighting (check the corresponding scikit-learn documentation).</span>
<span class="sd">                Array of weights that are assigned to individual samples.</span>
<span class="sd">                Note that they&#39;re only used for fitting of  the model, not during evaluation of metrics.</span>
<span class="sd">                If not provided, then each sample is given unit weight.</span>

<span class="sd">            model:</span>
<span class="sd">                Classifier or regressor to be fitted on the train folds.</span>

<span class="sd">            train_index (np.array):</span>
<span class="sd">                Positions of train folds samples.</span>

<span class="sd">            val_index (np.array):</span>
<span class="sd">                Positions of validation fold samples.</span>

<span class="sd">            **shap_kwargs:</span>
<span class="sd">                keyword arguments passed to</span>
<span class="sd">                [shap.Explainer](https://shap.readthedocs.io/en/latest/generated/shap.Explainer.html#shap.Explainer).</span>
<span class="sd">                It also enables `approximate` and `check_additivity` parameters, passed while calculating SHAP values.</span>
<span class="sd">                The `approximate=True` causes less accurate, but faster SHAP values calculation, while</span>
<span class="sd">                `check_additivity=False` disables the additivity check inside SHAP.</span>
<span class="sd">        Returns:</span>
<span class="sd">            (np.array, float, float):</span>
<span class="sd">                Tuple with the results: Shap Values on validation fold, train score, validation score.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">X_train</span><span class="p">,</span> <span class="n">X_val</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">train_index</span><span class="p">,</span> <span class="p">:],</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">val_index</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">y_train</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">val_index</span><span class="p">]</span>

        <span class="n">fit_params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_fit_params</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">X_train</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span>
            <span class="n">y_train</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span>
            <span class="n">X_val</span><span class="o">=</span><span class="n">X_val</span><span class="p">,</span>
            <span class="n">y_val</span><span class="o">=</span><span class="n">y_val</span><span class="p">,</span>
            <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
            <span class="n">train_index</span><span class="o">=</span><span class="n">train_index</span><span class="p">,</span>
            <span class="n">val_index</span><span class="o">=</span><span class="n">val_index</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="kn">from</span> <span class="nn">xgboost.sklearn</span> <span class="kn">import</span> <span class="n">XGBModel</span>

            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">XGBModel</span><span class="p">):</span>
                <span class="n">model</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">eval_metric</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">eval_metric</span><span class="p">,</span> <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">early_stopping_rounds</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
            <span class="k">pass</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="kn">from</span> <span class="nn">catboost</span> <span class="kn">import</span> <span class="n">CatBoost</span>

            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">CatBoost</span><span class="p">):</span>
                <span class="n">model</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">early_stopping_rounds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">early_stopping_rounds</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
            <span class="k">pass</span>

        <span class="c1"># Train the model</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="o">**</span><span class="n">fit_params</span><span class="p">)</span>

        <span class="c1"># Score the model</span>
        <span class="n">score_train</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scorer</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
        <span class="n">score_val</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scorer</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">)</span>

        <span class="c1"># Compute SHAP values</span>
        <span class="n">shap_values</span> <span class="o">=</span> <span class="n">shap_calc</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span> <span class="o">**</span><span class="n">shap_kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">shap_values</span><span class="p">,</span> <span class="n">score_train</span><span class="p">,</span> <span class="n">score_val</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="probatus.feature_elimination.feature_elimination.ShapRFECV.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">min_features_to_select</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">eval_metric</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

      <p>This method initializes the class.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>model</code></td>
            <td>
                  <code>classifier or regressor, sklearn compatible search CV e.g. GridSearchCV, RandomizedSearchCV or BayesSearchCV</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A model that will be optimized and trained at each round of feature elimination. The recommended model
is <a href="https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html">LGBMClassifier</a>,
because it by default handles the missing values and categorical variables. This parameter also supports
any hyperparameter search schema that is consistent with the sklearn API e.g.
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html">GridSearchCV</a>,
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html">RandomizedSearchCV</a>
or <a href="https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html#skopt.BayesSearchCV">BayesSearchCV</a>.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>step</code></td>
            <td>
                  <code>int or float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of lowest importance features removed each round. If it is an int, then each round such a number of
features are discarded. If float, such a percentage of remaining features (rounded down) is removed each
iteration. It is recommended to use float, since it is faster for a large number of features, and slows
down and becomes more precise with fewer features. Note: the last round may remove fewer features in
order to reach min_features_to_select.
If columns_to_keep parameter is specified in the fit method, step is the number of features to remove after
keeping those columns.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>min_features_to_select</code></td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Minimum number of features to be kept. This is a stopping criterion of the feature elimination. By
default the process stops when one feature is left. If columns_to_keep is specified in the fit method,
it may override this parameter to the maximum between length of columns_to_keep the two.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>cv</code></td>
            <td>
                  <code>int, cross-validation generator or an iterable</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Determines the cross-validation splitting strategy. Compatible with sklearn
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html">cv parameter</a>.
If None, then cv of 5 is used.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>scoring</code></td>
            <td>
                  <code>string or <a class="autorefs autorefs-internal" title="probatus.utils.Scorer" href="utils.html#probatus.utils.scoring.Scorer">Scorer</a></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Metric for which the model performance is calculated. It can be either a metric name aligned with predefined
<a href="https://scikit-learn.org/stable/modules/model_evaluation.html">classification scorers names in sklearn</a>.</p>
              </div>
            </td>
            <td>
                  <code>&#39;roc_auc&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>n_jobs</code></td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of cores to run in parallel while fitting across folds. None means 1 unless in a
<code>joblib.parallel_backend</code> context. -1 means using all processors.</p>
              </div>
            </td>
            <td>
                  <code>-1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>verbose</code></td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Controls verbosity of the output:</p>
<ul>
<li>0 - neither prints nor warnings are shown</li>
<li>1 - only most important warnings</li>
<li>2 - shows all prints and all warnings.</li>
</ul>
              </div>
            </td>
            <td>
                  <code>0</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>random_state</code></td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Random state set at each round of feature elimination. If it is None, the results will not be
reproducible and in random search at each iteration a different hyperparameters might be tested. For
reproducible results set it to an integer.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>early_stopping_rounds</code></td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of rounds with constant performance after which the model fitting stops. This is passed to the
fit method of the model for Shapley values estimation, but not for hyperparameter search. Only
supported by some models, such as XGBoost, LightGBM and CatBoost. Only recommended when dealing with large sets of data.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>eval_metric</code></td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Metric for scoring fitting rounds and activating early stopping. This is passed to the
fit method of the model for Shapley values estimation, but not for hyperparameter search. Only
supported by some models, such as <a href="https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters">XGBoost</a>
and <a href="https://lightgbm.readthedocs.io/en/latest/Parameters.html#metric-parameters">LightGBM</a>.
Note that <code>eval_metric</code> is an argument of the model's fit method and it is different from <code>scoring</code>.
Only recommended when dealing with large sets of data.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>probatus/feature_elimination/feature_elimination.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">min_features_to_select</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">cv</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">scoring</span><span class="o">=</span><span class="s2">&quot;roc_auc&quot;</span><span class="p">,</span>
    <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">eval_metric</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This method initializes the class.</span>

<span class="sd">    Args:</span>
<span class="sd">        model (classifier or regressor, sklearn compatible search CV e.g. GridSearchCV, RandomizedSearchCV or BayesSearchCV):</span>
<span class="sd">            A model that will be optimized and trained at each round of feature elimination. The recommended model</span>
<span class="sd">            is [LGBMClassifier](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html),</span>
<span class="sd">            because it by default handles the missing values and categorical variables. This parameter also supports</span>
<span class="sd">            any hyperparameter search schema that is consistent with the sklearn API e.g.</span>
<span class="sd">            [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html),</span>
<span class="sd">            [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)</span>
<span class="sd">            or [BayesSearchCV](https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html#skopt.BayesSearchCV).</span>

<span class="sd">        step (int or float, optional):</span>
<span class="sd">            Number of lowest importance features removed each round. If it is an int, then each round such a number of</span>
<span class="sd">            features are discarded. If float, such a percentage of remaining features (rounded down) is removed each</span>
<span class="sd">            iteration. It is recommended to use float, since it is faster for a large number of features, and slows</span>
<span class="sd">            down and becomes more precise with fewer features. Note: the last round may remove fewer features in</span>
<span class="sd">            order to reach min_features_to_select.</span>
<span class="sd">            If columns_to_keep parameter is specified in the fit method, step is the number of features to remove after</span>
<span class="sd">            keeping those columns.</span>

<span class="sd">        min_features_to_select (int, optional):</span>
<span class="sd">            Minimum number of features to be kept. This is a stopping criterion of the feature elimination. By</span>
<span class="sd">            default the process stops when one feature is left. If columns_to_keep is specified in the fit method,</span>
<span class="sd">            it may override this parameter to the maximum between length of columns_to_keep the two.</span>

<span class="sd">        cv (int, cross-validation generator or an iterable, optional):</span>
<span class="sd">            Determines the cross-validation splitting strategy. Compatible with sklearn</span>
<span class="sd">            [cv parameter](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html).</span>
<span class="sd">            If None, then cv of 5 is used.</span>

<span class="sd">        scoring (string or probatus.utils.Scorer, optional):</span>
<span class="sd">            Metric for which the model performance is calculated. It can be either a metric name aligned with predefined</span>
<span class="sd">            [classification scorers names in sklearn](https://scikit-learn.org/stable/modules/model_evaluation.html).</span>

<span class="sd">        n_jobs (int, optional):</span>
<span class="sd">            Number of cores to run in parallel while fitting across folds. None means 1 unless in a</span>
<span class="sd">            `joblib.parallel_backend` context. -1 means using all processors.</span>

<span class="sd">        verbose (int, optional):</span>
<span class="sd">            Controls verbosity of the output:</span>

<span class="sd">            - 0 - neither prints nor warnings are shown</span>
<span class="sd">            - 1 - only most important warnings</span>
<span class="sd">            - 2 - shows all prints and all warnings.</span>

<span class="sd">        random_state (int, optional):</span>
<span class="sd">            Random state set at each round of feature elimination. If it is None, the results will not be</span>
<span class="sd">            reproducible and in random search at each iteration a different hyperparameters might be tested. For</span>
<span class="sd">            reproducible results set it to an integer.</span>

<span class="sd">        early_stopping_rounds (int, optional):</span>
<span class="sd">            Number of rounds with constant performance after which the model fitting stops. This is passed to the</span>
<span class="sd">            fit method of the model for Shapley values estimation, but not for hyperparameter search. Only</span>
<span class="sd">            supported by some models, such as XGBoost, LightGBM and CatBoost. Only recommended when dealing with large sets of data.</span>

<span class="sd">        eval_metric (str, optional):</span>
<span class="sd">            Metric for scoring fitting rounds and activating early stopping. This is passed to the</span>
<span class="sd">            fit method of the model for Shapley values estimation, but not for hyperparameter search. Only</span>
<span class="sd">            supported by some models, such as [XGBoost](https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters)</span>
<span class="sd">            and [LightGBM](https://lightgbm.readthedocs.io/en/latest/Parameters.html#metric-parameters).</span>
<span class="sd">            Note that `eval_metric` is an argument of the model&#39;s fit method and it is different from `scoring`.</span>
<span class="sd">            Only recommended when dealing with large sets of data.</span>
<span class="sd">    &quot;&quot;&quot;</span>  <span class="c1"># noqa</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">search_model</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">BaseSearchCV</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_step</span><span class="p">(</span><span class="n">step</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">min_features_to_select</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_min_features</span><span class="p">(</span><span class="n">min_features_to_select</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cv</span> <span class="o">=</span> <span class="n">cv</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">scorer</span> <span class="o">=</span> <span class="n">get_single_scorer</span><span class="p">(</span><span class="n">scoring</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span> <span class="o">=</span> <span class="n">n_jobs</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>

    <span class="c1"># Enable early stopping behavior</span>
    <span class="k">if</span> <span class="n">early_stopping_rounds</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">eval_metric</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Running early stopping, requires both &#39;early_stopping_rounds&#39; and &#39;eval_metric&#39; as&quot;</span>
                <span class="s2">&quot; parameters to be provided and supports only &#39;XGBoost&#39;, &#39;LGBM&#39; and &#39;CatBoost&#39;.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">early_stopping_rounds</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="n">early_stopping_rounds</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;early_stopping_rounds must be a positive integer; got </span><span class="si">{</span><span class="n">early_stopping_rounds</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_if_model_is_compatible_with_early_stopping</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Only &#39;XGBoost&#39;, &#39;LGBM&#39; and &#39;CatBoost&#39; supported for early stopping.&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">early_stopping_rounds</span> <span class="o">=</span> <span class="n">early_stopping_rounds</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">eval_metric</span> <span class="o">=</span> <span class="n">eval_metric</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="probatus.feature_elimination.feature_elimination.ShapRFECV.compute" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">compute</span><span class="p">()</span></code>

</h3>


    <div class="doc doc-contents ">

      <p>Checks if fit() method has been run.</p>
<p>and computes the DataFrame with results of feature elimination for each round.</p>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="pandas.DataFrame">DataFrame</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>DataFrame with results of feature elimination for each round.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>probatus/feature_elimination/feature_elimination.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">compute</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Checks if fit() method has been run.</span>

<span class="sd">    and computes the DataFrame with results of feature elimination for each round.</span>

<span class="sd">    Returns:</span>
<span class="sd">        (pd.DataFrame):</span>
<span class="sd">            DataFrame with results of feature elimination for each round.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_check_if_fitted</span><span class="p">()</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="probatus.feature_elimination.feature_elimination.ShapRFECV.fit" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">columns_to_keep</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">column_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">shap_variance_penalty_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">shap_kwargs</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

      <p>Fits the object with the provided data.</p>
<p>The algorithm starts with the entire dataset, and then sequentially
    eliminates features. If sklearn compatible search CV is passed as model e.g.
    <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html">GridSearchCV</a>,
    <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html">RandomizedSearchCV</a>
    or <a href="https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html">BayesSearchCV</a>,
    the hyperparameter optimization is applied at each step of the elimination.
    Then, the SHAP feature importance is calculated using Cross-Validation,
    and <code>step</code> lowest importance features are removed.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>X</code></td>
            <td>
                  <code><span title="pandas.DataFrame">DataFrame</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Provided dataset.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>y</code></td>
            <td>
                  <code><span title="pandas.Series">Series</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Labels for X.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>sample_weight</code></td>
            <td>
                  <code>(<span title="pandas.Series">Series</span>, <span title="numpy.ndarray">ndarray</span>, list)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>array-like of shape (n_samples,) - only use if the model you're using supports
sample weighting (check the corresponding scikit-learn documentation).
Array of weights that are assigned to individual samples.
Note that they're only used for fitting of  the model, not during evaluation of metrics.
If not provided, then each sample is given unit weight.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>columns_to_keep</code></td>
            <td>
                  <code>list of str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List of column names to keep. If given,
these columns will not be eliminated by the feature elimination process.
However, these feature will used for the calculation of the SHAP values.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>column_names</code></td>
            <td>
                  <code>list of str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List of feature names of the provided samples. If provided it will be used to overwrite the existing
feature names. If not provided the existing feature names are used or default feature names are
generated.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>groups</code></td>
            <td>
                  <code>(<span title="pandas.Series">Series</span>, <span title="numpy.ndarray">ndarray</span>, list)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>array-like of shape (n_samples,)
Group labels for the samples used while splitting the dataset into train/test set.
Only used in conjunction with a "Group" <code>cv</code> instance.
(e.g. <code>sklearn.model_selection.GroupKFold</code>).</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>shap_variance_penalty_factor</code></td>
            <td>
                  <code>int or float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Apply aggregation penalty when computing average of shap values for a given feature.
Results in a preference for features that have smaller standard deviation of shap
values (more coherent shap importance). Recommend value 0.5 - 1.0.
Formula: penalized_shap_mean = (mean_shap - (std_shap * shap_variance_penalty_factor))</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>**shap_kwargs</code></td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>keyword arguments passed to
<a href="https://shap.readthedocs.io/en/latest/generated/shap.Explainer.html#shap.Explainer">shap.Explainer</a>.
It also enables <code>approximate</code> and <code>check_additivity</code> parameters, passed while calculating SHAP values.
The <code>approximate=True</code> causes less accurate, but faster SHAP values calculation, while
<code>check_additivity=False</code> disables the additivity check inside SHAP.</p>
              </div>
            </td>
            <td>
                  <code>{}</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><a class="autorefs autorefs-internal" title="probatus.feature_elimination.feature_elimination.ShapRFECV" href="#probatus.feature_elimination.feature_elimination.ShapRFECV">ShapRFECV</a></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Fitted object.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>probatus/feature_elimination/feature_elimination.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">X</span><span class="p">,</span>
    <span class="n">y</span><span class="p">,</span>
    <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">columns_to_keep</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">column_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">groups</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">shap_variance_penalty_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">shap_kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fits the object with the provided data.</span>

<span class="sd">    The algorithm starts with the entire dataset, and then sequentially</span>
<span class="sd">        eliminates features. If sklearn compatible search CV is passed as model e.g.</span>
<span class="sd">        [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html),</span>
<span class="sd">        [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)</span>
<span class="sd">        or [BayesSearchCV](https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html),</span>
<span class="sd">        the hyperparameter optimization is applied at each step of the elimination.</span>
<span class="sd">        Then, the SHAP feature importance is calculated using Cross-Validation,</span>
<span class="sd">        and `step` lowest importance features are removed.</span>

<span class="sd">    Args:</span>
<span class="sd">        X (pd.DataFrame):</span>
<span class="sd">            Provided dataset.</span>

<span class="sd">        y (pd.Series):</span>
<span class="sd">            Labels for X.</span>

<span class="sd">        sample_weight (pd.Series, np.ndarray, list, optional):</span>
<span class="sd">            array-like of shape (n_samples,) - only use if the model you&#39;re using supports</span>
<span class="sd">            sample weighting (check the corresponding scikit-learn documentation).</span>
<span class="sd">            Array of weights that are assigned to individual samples.</span>
<span class="sd">            Note that they&#39;re only used for fitting of  the model, not during evaluation of metrics.</span>
<span class="sd">            If not provided, then each sample is given unit weight.</span>

<span class="sd">        columns_to_keep (list of str, optional):</span>
<span class="sd">            List of column names to keep. If given,</span>
<span class="sd">            these columns will not be eliminated by the feature elimination process.</span>
<span class="sd">            However, these feature will used for the calculation of the SHAP values.</span>

<span class="sd">        column_names (list of str, optional):</span>
<span class="sd">            List of feature names of the provided samples. If provided it will be used to overwrite the existing</span>
<span class="sd">            feature names. If not provided the existing feature names are used or default feature names are</span>
<span class="sd">            generated.</span>

<span class="sd">        groups (pd.Series, np.ndarray, list, optional):</span>
<span class="sd">            array-like of shape (n_samples,)</span>
<span class="sd">            Group labels for the samples used while splitting the dataset into train/test set.</span>
<span class="sd">            Only used in conjunction with a &quot;Group&quot; `cv` instance.</span>
<span class="sd">            (e.g. `sklearn.model_selection.GroupKFold`).</span>

<span class="sd">        shap_variance_penalty_factor (int or float, optional):</span>
<span class="sd">            Apply aggregation penalty when computing average of shap values for a given feature.</span>
<span class="sd">            Results in a preference for features that have smaller standard deviation of shap</span>
<span class="sd">            values (more coherent shap importance). Recommend value 0.5 - 1.0.</span>
<span class="sd">            Formula: penalized_shap_mean = (mean_shap - (std_shap * shap_variance_penalty_factor))</span>

<span class="sd">        **shap_kwargs:</span>
<span class="sd">            keyword arguments passed to</span>
<span class="sd">            [shap.Explainer](https://shap.readthedocs.io/en/latest/generated/shap.Explainer.html#shap.Explainer).</span>
<span class="sd">            It also enables `approximate` and `check_additivity` parameters, passed while calculating SHAP values.</span>
<span class="sd">            The `approximate=True` causes less accurate, but faster SHAP values calculation, while</span>
<span class="sd">            `check_additivity=False` disables the additivity check inside SHAP.</span>

<span class="sd">    Returns:</span>
<span class="sd">        (ShapRFECV): Fitted object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Initialise len_columns_to_keep based on columns_to_keep content validation</span>
    <span class="n">len_columns_to_keep</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="n">columns_to_keep</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">columns_to_keep</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;All elements in columns_to_keep must be strings.&quot;</span><span class="p">)</span>
        <span class="n">len_columns_to_keep</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">columns_to_keep</span><span class="p">)</span>

    <span class="c1"># Validate matching column names, if both columns_to_keep and column_names are provided</span>
    <span class="k">if</span> <span class="n">column_names</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="n">x</span> <span class="ow">in</span> <span class="n">column_names</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Column names in columns_to_keep and column_names do not match.&quot;</span><span class="p">)</span>

    <span class="c1"># Validate total number of columns to select against the total number of columns</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="n">column_names</span>
        <span class="ow">and</span> <span class="n">columns_to_keep</span>
        <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_features_to_select</span> <span class="o">+</span> <span class="n">len_columns_to_keep</span><span class="p">)</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">column_names</span><span class="p">)</span>
    <span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Minimum features to select plus columns_to_keep exceeds total number of features.&quot;</span><span class="p">)</span>

    <span class="c1"># Check shap_variance_penalty_factor has acceptable value</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shap_variance_penalty_factor</span><span class="p">,</span> <span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">))</span> <span class="ow">and</span> <span class="n">shap_variance_penalty_factor</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">_shap_variance_penalty_factor</span> <span class="o">=</span> <span class="n">shap_variance_penalty_factor</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">shap_variance_penalty_factor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;shap_variance_penalty_factor must be None, int or float. Setting shap_variance_penalty_factor = 0&quot;</span>
            <span class="p">)</span>
        <span class="n">_shap_variance_penalty_factor</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">column_names</span> <span class="o">=</span> <span class="n">preprocess_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X_name</span><span class="o">=</span><span class="s2">&quot;X&quot;</span><span class="p">,</span> <span class="n">column_names</span><span class="o">=</span><span class="n">column_names</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">preprocess_labels</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_name</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;sample_weight is passed only to the fit method of the model, not the evaluation metrics.&quot;</span>
            <span class="p">)</span>
        <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">assure_pandas_series</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cv</span> <span class="o">=</span> <span class="n">check_cv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cv</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">,</span> <span class="n">classifier</span><span class="o">=</span><span class="n">is_classifier</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">))</span>

    <span class="n">remaining_features</span> <span class="o">=</span> <span class="n">current_features_set</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">column_names</span>
    <span class="n">round_number</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># Stop when stopping criteria is met.</span>
    <span class="n">stopping_criteria</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">min_features_to_select</span><span class="p">,</span> <span class="n">len_columns_to_keep</span><span class="p">])</span>

    <span class="c1"># Setting up the min_features_to_select parameter.</span>
    <span class="k">if</span> <span class="n">columns_to_keep</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_features_to_select</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="c1"># Ensures that, if columns_to_keep is provided, the last features remaining are only the columns_to_keep.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Minimum features to select : </span><span class="si">{</span><span class="n">stopping_criteria</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">current_features_set</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">stopping_criteria</span><span class="p">:</span>
        <span class="n">round_number</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c1"># Get current dataset info</span>
        <span class="n">current_features_set</span> <span class="o">=</span> <span class="n">remaining_features</span>
        <span class="n">remaining_removeable_features</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">dict</span><span class="o">.</span><span class="n">fromkeys</span><span class="p">(</span><span class="n">current_features_set</span> <span class="o">+</span> <span class="p">(</span><span class="n">columns_to_keep</span> <span class="ow">or</span> <span class="p">[])))</span>

        <span class="c1"># Current dataset</span>
        <span class="n">current_X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">[</span><span class="n">remaining_removeable_features</span><span class="p">]</span>

        <span class="c1"># Optimize parameters</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">search_model</span><span class="p">:</span>
            <span class="n">current_search_model</span> <span class="o">=</span> <span class="n">clone</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">current_X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">)</span>
            <span class="n">current_model</span> <span class="o">=</span> <span class="n">current_search_model</span><span class="o">.</span><span class="n">estimator</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="n">current_search_model</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">current_model</span> <span class="o">=</span> <span class="n">clone</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>

        <span class="c1"># Early stopping enabled (or not)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">early_stopping_rounds</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_metric</span><span class="p">):</span>
            <span class="c1"># Perform CV to estimate feature importance with SHAP</span>
            <span class="n">results_per_fold</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">)(</span>
                <span class="n">delayed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_get_feature_shap_values_per_fold</span><span class="p">)(</span>
                    <span class="n">X</span><span class="o">=</span><span class="n">current_X</span><span class="p">,</span>
                    <span class="n">y</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">,</span>
                    <span class="n">model</span><span class="o">=</span><span class="n">current_model</span><span class="p">,</span>
                    <span class="n">train_index</span><span class="o">=</span><span class="n">train_index</span><span class="p">,</span>
                    <span class="n">val_index</span><span class="o">=</span><span class="n">val_index</span><span class="p">,</span>
                    <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
                    <span class="o">**</span><span class="n">shap_kwargs</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">val_index</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">cv</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">current_X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">,</span> <span class="n">groups</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Perform CV to estimate feature importance with SHAP</span>
            <span class="n">results_per_fold</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">)(</span>
                <span class="n">delayed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_get_feature_shap_values_per_fold_early_stopping</span><span class="p">)(</span>
                    <span class="n">X</span><span class="o">=</span><span class="n">current_X</span><span class="p">,</span>
                    <span class="n">y</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">,</span>
                    <span class="n">model</span><span class="o">=</span><span class="n">current_model</span><span class="p">,</span>
                    <span class="n">train_index</span><span class="o">=</span><span class="n">train_index</span><span class="p">,</span>
                    <span class="n">val_index</span><span class="o">=</span><span class="n">val_index</span><span class="p">,</span>
                    <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
                    <span class="o">**</span><span class="n">shap_kwargs</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">val_index</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">cv</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">current_X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">,</span> <span class="n">groups</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="o">.</span><span class="n">nunique</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">or</span> <span class="n">is_regressor</span><span class="p">(</span><span class="n">current_model</span><span class="p">):</span>
            <span class="n">shap_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">current_result</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">current_result</span> <span class="ow">in</span> <span class="n">results_per_fold</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># multi-class case</span>
            <span class="n">shap_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">current_result</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">current_result</span> <span class="ow">in</span> <span class="n">results_per_fold</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">scores_train</span> <span class="o">=</span> <span class="p">[</span><span class="n">current_result</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">current_result</span> <span class="ow">in</span> <span class="n">results_per_fold</span><span class="p">]</span>
        <span class="n">scores_val</span> <span class="o">=</span> <span class="p">[</span><span class="n">current_result</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">current_result</span> <span class="ow">in</span> <span class="n">results_per_fold</span><span class="p">]</span>

        <span class="c1"># Calculate the shap features with remaining features and features to keep.</span>
        <span class="n">shap_importance_df</span> <span class="o">=</span> <span class="n">calculate_shap_importance</span><span class="p">(</span>
            <span class="n">shap_values</span><span class="p">,</span> <span class="n">remaining_removeable_features</span><span class="p">,</span> <span class="n">shap_variance_penalty_factor</span><span class="o">=</span><span class="n">_shap_variance_penalty_factor</span>
        <span class="p">)</span>

        <span class="c1"># Determine which features to keep and which to remove.</span>
        <span class="n">remaining_features</span><span class="p">,</span> <span class="n">features_to_remove</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_filter_and_identify_features_based_on_importance</span><span class="p">(</span>
            <span class="n">shap_importance_df</span><span class="p">,</span> <span class="n">columns_to_keep</span><span class="p">,</span> <span class="n">current_features_set</span>
        <span class="p">)</span>

        <span class="c1"># Report results</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_report_current_results</span><span class="p">(</span>
            <span class="n">round_number</span><span class="o">=</span><span class="n">round_number</span><span class="p">,</span>
            <span class="n">current_features_set</span><span class="o">=</span><span class="n">current_features_set</span><span class="p">,</span>
            <span class="n">features_to_remove</span><span class="o">=</span><span class="n">features_to_remove</span><span class="p">,</span>
            <span class="n">train_metric_mean</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores_train</span><span class="p">),</span>
            <span class="n">train_metric_std</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">scores_train</span><span class="p">),</span>
            <span class="n">val_metric_mean</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores_val</span><span class="p">),</span>
            <span class="n">val_metric_std</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">scores_val</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Round: </span><span class="si">{</span><span class="n">round_number</span><span class="si">}</span><span class="s2">, Current number of features: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">current_features_set</span><span class="p">)</span><span class="si">}</span><span class="s2">, &quot;</span>
                <span class="sa">f</span><span class="s1">&#39;Current performance: Train </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">round_number</span><span class="p">][</span><span class="s2">&quot;train_metric_mean&quot;</span><span class="p">]</span><span class="si">}</span><span class="s1"> &#39;</span>
                <span class="sa">f</span><span class="s1">&#39;+/- </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">round_number</span><span class="p">][</span><span class="s2">&quot;train_metric_std&quot;</span><span class="p">]</span><span class="si">}</span><span class="s1">, CV Validation &#39;</span>
                <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">round_number</span><span class="p">][</span><span class="s2">&quot;val_metric_mean&quot;</span><span class="p">]</span><span class="si">}</span><span class="s1"> &#39;</span>
                <span class="sa">f</span><span class="s1">&#39;+/- </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">round_number</span><span class="p">][</span><span class="s2">&quot;val_metric_std&quot;</span><span class="p">]</span><span class="si">}</span><span class="s1">. </span><span class="se">\n</span><span class="s1">&#39;</span>
                <span class="sa">f</span><span class="s2">&quot;Features left: </span><span class="si">{</span><span class="n">remaining_features</span><span class="si">}</span><span class="s2">. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Removed features at the end of the round: </span><span class="si">{</span><span class="n">features_to_remove</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fitted</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="probatus.feature_elimination.feature_elimination.ShapRFECV.fit_compute" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">fit_compute</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">columns_to_keep</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">column_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">shap_variance_penalty_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">shap_kwargs</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

      <p>Fits the object with the provided data.</p>
<p>The algorithm starts with the entire dataset, and then sequentially
    eliminates features. If sklearn compatible search CV is passed as model e.g.
    <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html">GridSearchCV</a>,
    <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html">RandomizedSearchCV</a>
    or <a href="https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html">BayesSearchCV</a>,
    the hyperparameter optimization is applied at each step of the elimination.
    Then, the SHAP feature importance is calculated using Cross-Validation,
    and <code>step</code> lowest importance features are removed. At the end, the
    report containing results from each iteration is computed and returned to the user.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>X</code></td>
            <td>
                  <code><span title="pandas.DataFrame">DataFrame</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Provided dataset.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>y</code></td>
            <td>
                  <code><span title="pandas.Series">Series</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Labels for X.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>sample_weight</code></td>
            <td>
                  <code>(<span title="pandas.Series">Series</span>, <span title="numpy.ndarray">ndarray</span>, list)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>array-like of shape (n_samples,) - only use if the model you're using supports
sample weighting (check the corresponding scikit-learn documentation).
Array of weights that are assigned to individual samples.
Note that they're only used for fitting of  the model, not during evaluation of metrics.
If not provided, then each sample is given unit weight.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>columns_to_keep</code></td>
            <td>
                  <code>list of str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List of columns to keep. If given, these columns will not be eliminated.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>column_names</code></td>
            <td>
                  <code>list of str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List of feature names of the provided samples. If provided it will be used to overwrite the existing
feature names. If not provided the existing feature names are used or default feature names are
generated.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>shap_variance_penalty_factor</code></td>
            <td>
                  <code>int or float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Apply aggregation penalty when computing average of shap values for a given feature.
Results in a preference for features that have smaller standard deviation of shap
values (more coherent shap importance). Recommend value 0.5 - 1.0.
Formula: penalized_shap_mean = (mean_shap - (std_shap * shap_variance_penalty_factor))</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>**shap_kwargs</code></td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>keyword arguments passed to
<a href="https://shap.readthedocs.io/en/latest/generated/shap.Explainer.html#shap.Explainer">shap.Explainer</a>.
It also enables <code>approximate</code> and <code>check_additivity</code> parameters, passed while calculating SHAP values.
The <code>approximate=True</code> causes less accurate, but faster SHAP values calculation, while
<code>check_additivity=False</code> disables the additivity check inside SHAP.</p>
              </div>
            </td>
            <td>
                  <code>{}</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="pandas.DataFrame">DataFrame</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>DataFrame containing results of feature elimination from each iteration.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>probatus/feature_elimination/feature_elimination.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">fit_compute</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">X</span><span class="p">,</span>
    <span class="n">y</span><span class="p">,</span>
    <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">columns_to_keep</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">column_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">shap_variance_penalty_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">shap_kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fits the object with the provided data.</span>

<span class="sd">    The algorithm starts with the entire dataset, and then sequentially</span>
<span class="sd">        eliminates features. If sklearn compatible search CV is passed as model e.g.</span>
<span class="sd">        [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html),</span>
<span class="sd">        [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)</span>
<span class="sd">        or [BayesSearchCV](https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html),</span>
<span class="sd">        the hyperparameter optimization is applied at each step of the elimination.</span>
<span class="sd">        Then, the SHAP feature importance is calculated using Cross-Validation,</span>
<span class="sd">        and `step` lowest importance features are removed. At the end, the</span>
<span class="sd">        report containing results from each iteration is computed and returned to the user.</span>

<span class="sd">    Args:</span>
<span class="sd">        X (pd.DataFrame):</span>
<span class="sd">            Provided dataset.</span>

<span class="sd">        y (pd.Series):</span>
<span class="sd">            Labels for X.</span>

<span class="sd">        sample_weight (pd.Series, np.ndarray, list, optional):</span>
<span class="sd">            array-like of shape (n_samples,) - only use if the model you&#39;re using supports</span>
<span class="sd">            sample weighting (check the corresponding scikit-learn documentation).</span>
<span class="sd">            Array of weights that are assigned to individual samples.</span>
<span class="sd">            Note that they&#39;re only used for fitting of  the model, not during evaluation of metrics.</span>
<span class="sd">            If not provided, then each sample is given unit weight.</span>

<span class="sd">        columns_to_keep (list of str, optional):</span>
<span class="sd">            List of columns to keep. If given, these columns will not be eliminated.</span>

<span class="sd">        column_names (list of str, optional):</span>
<span class="sd">            List of feature names of the provided samples. If provided it will be used to overwrite the existing</span>
<span class="sd">            feature names. If not provided the existing feature names are used or default feature names are</span>
<span class="sd">            generated.</span>

<span class="sd">        shap_variance_penalty_factor (int or float, optional):</span>
<span class="sd">            Apply aggregation penalty when computing average of shap values for a given feature.</span>
<span class="sd">            Results in a preference for features that have smaller standard deviation of shap</span>
<span class="sd">            values (more coherent shap importance). Recommend value 0.5 - 1.0.</span>
<span class="sd">            Formula: penalized_shap_mean = (mean_shap - (std_shap * shap_variance_penalty_factor))</span>

<span class="sd">        **shap_kwargs:</span>
<span class="sd">            keyword arguments passed to</span>
<span class="sd">            [shap.Explainer](https://shap.readthedocs.io/en/latest/generated/shap.Explainer.html#shap.Explainer).</span>
<span class="sd">            It also enables `approximate` and `check_additivity` parameters, passed while calculating SHAP values.</span>
<span class="sd">            The `approximate=True` causes less accurate, but faster SHAP values calculation, while</span>
<span class="sd">            `check_additivity=False` disables the additivity check inside SHAP.</span>

<span class="sd">    Returns:</span>
<span class="sd">        (pd.DataFrame):</span>
<span class="sd">            DataFrame containing results of feature elimination from each iteration.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
        <span class="n">X</span><span class="p">,</span>
        <span class="n">y</span><span class="p">,</span>
        <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
        <span class="n">columns_to_keep</span><span class="o">=</span><span class="n">columns_to_keep</span><span class="p">,</span>
        <span class="n">column_names</span><span class="o">=</span><span class="n">column_names</span><span class="p">,</span>
        <span class="n">shap_variance_penalty_factor</span><span class="o">=</span><span class="n">shap_variance_penalty_factor</span><span class="p">,</span>
        <span class="o">**</span><span class="n">shap_kwargs</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="probatus.feature_elimination.feature_elimination.ShapRFECV.get_reduced_features_set" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">get_reduced_features_set</span><span class="p">(</span><span class="n">num_features</span><span class="p">,</span> <span class="n">standard_error_threshold</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s1">&#39;feature_names&#39;</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

      <p>Gets the features set after the feature elimination process, for a given number of features.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>num_features</code></td>
            <td>
                  <code>int or str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If int: Number of features in the reduced features set.
If str: One of the following automatic num feature selection methods supported:
    1. best: strictly selects the num_features with the highest model score.
    2. best_coherent: For iterations that are within standard_error_threshold of the highest
    score, select the iteration with the lowest standard deviation of model score.
    3. best_parsimonious: For iterations that are within standard_error_threshold of the
    highest score, select the iteration with the fewest features.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>standard_error_threshold</code></td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If num_features is 'best_coherent' or 'best_parsimonious', this parameter is used.</p>
              </div>
            </td>
            <td>
                  <code>1.0</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>return_type</code></td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Accepts possible values of 'feature_names', 'support' or 'ranking'. These are defined as:
    1. feature_names: returns column names
    2. support: returns boolean mask
    3. ranking: returns numeric ranking of features</p>
              </div>
            </td>
            <td>
                  <code>&#39;feature_names&#39;</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code>list of str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Reduced features set.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>probatus/feature_elimination/feature_elimination.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">829</span>
<span class="normal">830</span>
<span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span>
<span class="normal">837</span>
<span class="normal">838</span>
<span class="normal">839</span>
<span class="normal">840</span>
<span class="normal">841</span>
<span class="normal">842</span>
<span class="normal">843</span>
<span class="normal">844</span>
<span class="normal">845</span>
<span class="normal">846</span>
<span class="normal">847</span>
<span class="normal">848</span>
<span class="normal">849</span>
<span class="normal">850</span>
<span class="normal">851</span>
<span class="normal">852</span>
<span class="normal">853</span>
<span class="normal">854</span>
<span class="normal">855</span>
<span class="normal">856</span>
<span class="normal">857</span>
<span class="normal">858</span>
<span class="normal">859</span>
<span class="normal">860</span>
<span class="normal">861</span>
<span class="normal">862</span>
<span class="normal">863</span>
<span class="normal">864</span>
<span class="normal">865</span>
<span class="normal">866</span>
<span class="normal">867</span>
<span class="normal">868</span>
<span class="normal">869</span>
<span class="normal">870</span>
<span class="normal">871</span>
<span class="normal">872</span>
<span class="normal">873</span>
<span class="normal">874</span>
<span class="normal">875</span>
<span class="normal">876</span>
<span class="normal">877</span>
<span class="normal">878</span>
<span class="normal">879</span>
<span class="normal">880</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_reduced_features_set</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_features</span><span class="p">,</span> <span class="n">standard_error_threshold</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s2">&quot;feature_names&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gets the features set after the feature elimination process, for a given number of features.</span>

<span class="sd">    Args:</span>
<span class="sd">        num_features (int or str):</span>
<span class="sd">            If int: Number of features in the reduced features set.</span>
<span class="sd">            If str: One of the following automatic num feature selection methods supported:</span>
<span class="sd">                1. best: strictly selects the num_features with the highest model score.</span>
<span class="sd">                2. best_coherent: For iterations that are within standard_error_threshold of the highest</span>
<span class="sd">                score, select the iteration with the lowest standard deviation of model score.</span>
<span class="sd">                3. best_parsimonious: For iterations that are within standard_error_threshold of the</span>
<span class="sd">                highest score, select the iteration with the fewest features.</span>

<span class="sd">        standard_error_threshold (float):</span>
<span class="sd">            If num_features is &#39;best_coherent&#39; or &#39;best_parsimonious&#39;, this parameter is used.</span>

<span class="sd">        return_type:</span>
<span class="sd">            Accepts possible values of &#39;feature_names&#39;, &#39;support&#39; or &#39;ranking&#39;. These are defined as:</span>
<span class="sd">                1. feature_names: returns column names</span>
<span class="sd">                2. support: returns boolean mask</span>
<span class="sd">                3. ranking: returns numeric ranking of features</span>

<span class="sd">    Returns:</span>
<span class="sd">        (list of str):</span>
<span class="sd">            Reduced features set.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_check_if_fitted</span><span class="p">()</span>

    <span class="c1"># Determine the best number of features based on the method specified</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">num_features</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">num_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_best_num_features</span><span class="p">(</span>
            <span class="n">best_method</span><span class="o">=</span><span class="n">num_features</span><span class="p">,</span> <span class="n">standard_error_threshold</span><span class="o">=</span><span class="n">standard_error_threshold</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">num_features</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Parameter num_features can be of type int, or of type str with &quot;</span>
            <span class="s2">&quot;possible values of &#39;best&#39;, &#39;best_coherent&#39; or &#39;best_parsimonious&#39;&quot;</span>
        <span class="p">)</span>

    <span class="c1"># Get feature names for the determined number of features</span>
    <span class="n">feature_names_selected</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_feature_names</span><span class="p">(</span><span class="n">num_features</span><span class="p">)</span>

    <span class="c1"># Return based on the requested return type</span>
    <span class="k">if</span> <span class="n">return_type</span> <span class="o">==</span> <span class="s2">&quot;feature_names&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">feature_names_selected</span>
    <span class="k">elif</span> <span class="n">return_type</span> <span class="o">==</span> <span class="s2">&quot;support&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_feature_support</span><span class="p">(</span><span class="n">feature_names_selected</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">return_type</span> <span class="o">==</span> <span class="s2">&quot;ranking&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_feature_ranking</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid return_type. Must be &#39;feature_names&#39;, &#39;support&#39;, or &#39;ranking&#39;.&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="probatus.feature_elimination.feature_elimination.ShapRFECV.plot" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">plot</span><span class="p">(</span><span class="n">show</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">figure_kwargs</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

      <p>Generates plot of the model performance for each iteration of feature elimination.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>show</code></td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If True, the plots are showed to the user, otherwise they are not shown. Not showing plot can be useful,
when you want to edit the returned figure, before showing it.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>**figure_kwargs</code></td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Keyword arguments that are passed to the plt.figure, at its initialization.</p>
              </div>
            </td>
            <td>
                  <code>{}</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="matplotlib.pyplot.figure">figure</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Figure containing the performance plot.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>probatus/feature_elimination/feature_elimination.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">figure_kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates plot of the model performance for each iteration of feature elimination.</span>

<span class="sd">    Args:</span>
<span class="sd">        show (bool, optional):</span>
<span class="sd">            If True, the plots are showed to the user, otherwise they are not shown. Not showing plot can be useful,</span>
<span class="sd">            when you want to edit the returned figure, before showing it.</span>

<span class="sd">        **figure_kwargs:</span>
<span class="sd">            Keyword arguments that are passed to the plt.figure, at its initialization.</span>

<span class="sd">    Returns:</span>
<span class="sd">        (plt.figure):</span>
<span class="sd">            Figure containing the performance plot.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Data preparation</span>
    <span class="n">num_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="p">[</span><span class="s2">&quot;num_features&quot;</span><span class="p">]</span>
    <span class="n">train_mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="p">[</span><span class="s2">&quot;train_metric_mean&quot;</span><span class="p">]</span>
    <span class="n">train_std</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="p">[</span><span class="s2">&quot;train_metric_std&quot;</span><span class="p">]</span>
    <span class="n">val_mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="p">[</span><span class="s2">&quot;val_metric_mean&quot;</span><span class="p">]</span>
    <span class="n">val_std</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">report_df</span><span class="p">[</span><span class="s2">&quot;val_metric_std&quot;</span><span class="p">]</span>
    <span class="n">x_ticks</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">num_features</span><span class="o">.</span><span class="n">tolist</span><span class="p">()))</span>

    <span class="c1"># Plotting</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="o">**</span><span class="n">figure_kwargs</span><span class="p">)</span>

    <span class="c1"># Training performance</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">num_features</span><span class="p">,</span> <span class="n">train_mean</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Train Score&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">num_features</span><span class="p">,</span> <span class="n">train_mean</span> <span class="o">-</span> <span class="n">train_std</span><span class="p">,</span> <span class="n">train_mean</span> <span class="o">+</span> <span class="n">train_std</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

    <span class="c1"># Validation performance</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">num_features</span><span class="p">,</span> <span class="n">val_mean</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Validation Score&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">num_features</span><span class="p">,</span> <span class="n">val_mean</span> <span class="o">-</span> <span class="n">val_std</span><span class="p">,</span> <span class="n">val_mean</span> <span class="o">+</span> <span class="n">val_std</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

    <span class="c1"># Labels and title</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Number of features&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Performance </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">scorer</span><span class="o">.</span><span class="n">metric_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Backwards Feature Elimination using SHAP &amp; CV&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;lower left&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">invert_xaxis</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">x_ticks</span><span class="p">)</span>

    <span class="c1"># Display or close plot</span>
    <span class="k">if</span> <span class="n">show</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">(</span><span class="n">fig</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">fig</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; ING Bank N.V.
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.tabs"], "search": "../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.fe8b6f2b.min.js"></script>
      
    
  </body>
</html>